[2024-11-23T15:12:37.827+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: transfer_to_bronze_layer_full.transfer_to_bronze_layer_full manual__2024-11-23T15:11:27.038456+00:00 [queued]>
[2024-11-23T15:12:37.848+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: transfer_to_bronze_layer_full.transfer_to_bronze_layer_full manual__2024-11-23T15:11:27.038456+00:00 [queued]>
[2024-11-23T15:12:37.850+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-11-23T15:12:37.901+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): transfer_to_bronze_layer_full> on 2024-11-23 15:11:27.038456+00:00
[2024-11-23T15:12:37.918+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'transfer_to_bronze_layer_full', 'transfer_to_bronze_layer_full', 'manual__2024-11-23T15:11:27.038456+00:00', '--job-id', '323', '--raw', '--subdir', 'DAGS_FOLDER/step_2_bronzen_layer_data_full.py', '--cfg-path', '/tmp/tmph09b7_7d']
[2024-11-23T15:12:37.944+0000] {standard_task_runner.py:85} INFO - Job 323: Subtask transfer_to_bronze_layer_full
[2024-11-23T15:12:37.927+0000] {standard_task_runner.py:57} INFO - Started process 1469 to run task
[2024-11-23T15:12:38.465+0000] {task_command.py:415} INFO - Running <TaskInstance: transfer_to_bronze_layer_full.transfer_to_bronze_layer_full manual__2024-11-23T15:11:27.038456+00:00 [running]> on host b5d2cc198f20
[2024-11-23T15:12:39.372+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='transfer_to_bronze_layer_full' AIRFLOW_CTX_TASK_ID='transfer_to_bronze_layer_full' AIRFLOW_CTX_EXECUTION_DATE='2024-11-23T15:11:27.038456+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-23T15:11:27.038456+00:00'
[2024-11-23T15:12:39.391+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2024-11-23T15:12:39.395+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'docker exec python_environment python ./source_code/transfer_data/transfer_data_CreditStagingDB_to_BronzeLayer.py']
[2024-11-23T15:12:39.434+0000] {subprocess.py:86} INFO - Output:
[2024-11-23T15:12:55.842+0000] {subprocess.py:93} INFO - 24/11/23 15:12:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-11-23T15:12:56.550+0000] {subprocess.py:93} INFO - Setting default log level to "WARN".
[2024-11-23T15:12:56.552+0000] {subprocess.py:93} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2024-11-23T15:13:23.041+0000] {subprocess.py:93} INFO - spark_submit_local_jars_str:
[2024-11-23T15:13:23.069+0000] {subprocess.py:93} INFO - 		 /container/pyspark_workspace/local_data_storage/spark/jars/delta-core_2.12-2.1.0.jar,/container/pyspark_workspace/local_data_storage/spark/jars/mssql-jdbc-12.8.1.jre11.jar,/container/pyspark_workspace/local_data_storage/spark/jars/spark-avro_2.12-3.4.0.jar,/container/pyspark_workspace/local_data_storage/spark/jars/spark-sql-kafka-0-10_2.12-3.4.0.jar
[2024-11-23T15:13:23.070+0000] {subprocess.py:93} INFO - spark_submit_jdbc_driver_str:
[2024-11-23T15:13:23.071+0000] {subprocess.py:93} INFO - 		 /container/pyspark_workspace/local_data_storage/spark/jars/mssql-jdbc-12.8.1.jre11.jar
[2024-11-23T15:13:23.072+0000] {subprocess.py:93} INFO - spark_submit_extensions_str:
[2024-11-23T15:13:23.073+0000] {subprocess.py:93} INFO - 		 io.delta.sql.DeltaSparkSessionExtension
[2024-11-23T15:13:23.073+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2024-11-23T15:13:23.075+0000] {subprocess.py:93} INFO -   File "/container/pyspark_workspace/./source_code/transfer_data/transfer_data_CreditStagingDB_to_BronzeLayer.py", line 15, in <module>
[2024-11-23T15:13:23.075+0000] {subprocess.py:93} INFO -     dbo_CustomerProductAccount_df.write.format('delta').mode('overwrite').save('/container/pyspark_workspace/local_data_storage/delta_lake/bronze/CustomerProductAccount')
[2024-11-23T15:13:23.076+0000] {subprocess.py:93} INFO -   File "/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py", line 1463, in save
[2024-11-23T15:13:23.077+0000] {subprocess.py:93} INFO -     self._jwrite.save(path)
[2024-11-23T15:13:23.078+0000] {subprocess.py:93} INFO -   File "/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py", line 1322, in __call__
[2024-11-23T15:13:23.079+0000] {subprocess.py:93} INFO -     return_value = get_return_value(
[2024-11-23T15:13:23.079+0000] {subprocess.py:93} INFO -                    ^^^^^^^^^^^^^^^^^
[2024-11-23T15:13:23.080+0000] {subprocess.py:93} INFO -   File "/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
[2024-11-23T15:13:23.081+0000] {subprocess.py:93} INFO -     return f(*a, **kw)
[2024-11-23T15:13:23.082+0000] {subprocess.py:93} INFO -            ^^^^^^^^^^^
[2024-11-23T15:13:23.082+0000] {subprocess.py:93} INFO -   File "/usr/local/lib/python3.11/site-packages/py4j/protocol.py", line 326, in get_return_value
[2024-11-23T15:13:23.083+0000] {subprocess.py:93} INFO -     raise Py4JJavaError(
[2024-11-23T15:13:23.169+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o54.save.
[2024-11-23T15:13:23.170+0000] {subprocess.py:93} INFO - : com.google.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: io/delta/storage/LogStore
[2024-11-23T15:13:23.170+0000] {subprocess.py:93} INFO - Please ensure that the delta-storage dependency is included.
[2024-11-23T15:13:23.171+0000] {subprocess.py:93} INFO - 
[2024-11-23T15:13:23.172+0000] {subprocess.py:93} INFO - If using Python, please ensure you call `configure_spark_with_delta_pip` or use
[2024-11-23T15:13:23.173+0000] {subprocess.py:93} INFO - `--packages io.delta:delta-core_<scala-version>:<delta-lake-version>`.
[2024-11-23T15:13:23.174+0000] {subprocess.py:93} INFO - See https://docs.delta.io/latest/quick-start.html#python.
[2024-11-23T15:13:23.175+0000] {subprocess.py:93} INFO - 
[2024-11-23T15:13:23.175+0000] {subprocess.py:93} INFO - More information about this dependency and how to include it can be found here:
[2024-11-23T15:13:23.176+0000] {subprocess.py:93} INFO - https://docs.delta.io/latest/porting.html#delta-lake-1-1-or-below-to-delta-lake-1-2-or-above.
[2024-11-23T15:13:23.177+0000] {subprocess.py:93} INFO - 
[2024-11-23T15:13:23.177+0000] {subprocess.py:93} INFO - 	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2261)
[2024-11-23T15:13:23.178+0000] {subprocess.py:93} INFO - 	at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
[2024-11-23T15:13:23.179+0000] {subprocess.py:93} INFO - 	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)
[2024-11-23T15:13:23.179+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:604)
[2024-11-23T15:13:23.180+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:611)
[2024-11-23T15:13:23.181+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:492)
[2024-11-23T15:13:23.182+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:151)
[2024-11-23T15:13:23.182+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
[2024-11-23T15:13:23.183+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2024-11-23T15:13:23.184+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2024-11-23T15:13:23.184+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2024-11-23T15:13:23.185+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
[2024-11-23T15:13:23.186+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2024-11-23T15:13:23.187+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2024-11-23T15:13:23.188+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2024-11-23T15:13:23.188+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2024-11-23T15:13:23.189+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2024-11-23T15:13:23.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
[2024-11-23T15:13:23.191+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2024-11-23T15:13:23.191+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
[2024-11-23T15:13:23.192+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
[2024-11-23T15:13:23.193+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
[2024-11-23T15:13:23.193+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
[2024-11-23T15:13:23.194+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2024-11-23T15:13:23.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2024-11-23T15:13:23.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2024-11-23T15:13:23.196+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2024-11-23T15:13:23.197+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
[2024-11-23T15:13:23.197+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
[2024-11-23T15:13:23.198+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
[2024-11-23T15:13:23.199+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
[2024-11-23T15:13:23.199+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
[2024-11-23T15:13:23.200+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
[2024-11-23T15:13:23.201+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
[2024-11-23T15:13:23.201+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:304)
[2024-11-23T15:13:23.202+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
[2024-11-23T15:13:23.203+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2024-11-23T15:13:23.204+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2024-11-23T15:13:23.204+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2024-11-23T15:13:23.205+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2024-11-23T15:13:23.206+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2024-11-23T15:13:23.207+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2024-11-23T15:13:23.207+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2024-11-23T15:13:23.208+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2024-11-23T15:13:23.209+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2024-11-23T15:13:23.209+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2024-11-23T15:13:23.210+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2024-11-23T15:13:23.211+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2024-11-23T15:13:23.212+0000] {subprocess.py:93} INFO - Caused by: java.lang.NoClassDefFoundError: io/delta/storage/LogStore
[2024-11-23T15:13:23.212+0000] {subprocess.py:93} INFO - Please ensure that the delta-storage dependency is included.
[2024-11-23T15:13:23.213+0000] {subprocess.py:93} INFO - 
[2024-11-23T15:13:23.214+0000] {subprocess.py:93} INFO - If using Python, please ensure you call `configure_spark_with_delta_pip` or use
[2024-11-23T15:13:23.214+0000] {subprocess.py:93} INFO - `--packages io.delta:delta-core_<scala-version>:<delta-lake-version>`.
[2024-11-23T15:13:23.215+0000] {subprocess.py:93} INFO - See https://docs.delta.io/latest/quick-start.html#python.
[2024-11-23T15:13:23.215+0000] {subprocess.py:93} INFO - 
[2024-11-23T15:13:23.216+0000] {subprocess.py:93} INFO - More information about this dependency and how to include it can be found here:
[2024-11-23T15:13:23.217+0000] {subprocess.py:93} INFO - https://docs.delta.io/latest/porting.html#delta-lake-1-1-or-below-to-delta-lake-1-2-or-above.
[2024-11-23T15:13:23.217+0000] {subprocess.py:93} INFO - 
[2024-11-23T15:13:23.218+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaErrorsBase.missingDeltaStorageJar(DeltaErrors.scala:2255)
[2024-11-23T15:13:23.218+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaErrorsBase.missingDeltaStorageJar$(DeltaErrors.scala:2252)
[2024-11-23T15:13:23.219+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaErrors$.missingDeltaStorageJar(DeltaErrors.scala:2293)
[2024-11-23T15:13:23.220+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.storage.DelegatingLogStore$.<init>(DelegatingLogStore.scala:163)
[2024-11-23T15:13:23.221+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.storage.DelegatingLogStore$.<clinit>(DelegatingLogStore.scala)
[2024-11-23T15:13:23.221+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.storage.DelegatingLogStore.$anonfun$schemeBasedLogStore$1(DelegatingLogStore.scala:67)
[2024-11-23T15:13:23.222+0000] {subprocess.py:93} INFO - 	at scala.Option.orElse(Option.scala:447)
[2024-11-23T15:13:23.223+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.storage.DelegatingLogStore.schemeBasedLogStore(DelegatingLogStore.scala:67)
[2024-11-23T15:13:23.224+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.storage.DelegatingLogStore.getDelegate(DelegatingLogStore.scala:85)
[2024-11-23T15:13:23.224+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.storage.DelegatingLogStore.read(DelegatingLogStore.scala:96)
[2024-11-23T15:13:23.225+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:373)
[2024-11-23T15:13:23.226+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2024-11-23T15:13:23.226+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2024-11-23T15:13:23.227+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:63)
[2024-11-23T15:13:23.228+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)
[2024-11-23T15:13:23.228+0000] {subprocess.py:93} INFO - 	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)
[2024-11-23T15:13:23.229+0000] {subprocess.py:93} INFO - 	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)
[2024-11-23T15:13:23.229+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:63)
[2024-11-23T15:13:23.230+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)
[2024-11-23T15:13:23.231+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)
[2024-11-23T15:13:23.231+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)
[2024-11-23T15:13:23.232+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:63)
[2024-11-23T15:13:23.233+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:372)
[2024-11-23T15:13:23.233+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:143)
[2024-11-23T15:13:23.234+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:142)
[2024-11-23T15:13:23.235+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:63)
[2024-11-23T15:13:23.235+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:371)
[2024-11-23T15:13:23.236+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.Checkpoints.lastCheckpoint(Checkpoints.scala:366)
[2024-11-23T15:13:23.236+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.Checkpoints.lastCheckpoint$(Checkpoints.scala:365)
[2024-11-23T15:13:23.237+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog.lastCheckpoint(DeltaLog.scala:63)
[2024-11-23T15:13:23.238+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:56)
[2024-11-23T15:13:23.239+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:68)
[2024-11-23T15:13:23.239+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:593)
[2024-11-23T15:13:23.240+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
[2024-11-23T15:13:23.241+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:589)
[2024-11-23T15:13:23.242+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)
[2024-11-23T15:13:23.242+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)
[2024-11-23T15:13:23.243+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:456)
[2024-11-23T15:13:23.244+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)
[2024-11-23T15:13:23.245+0000] {subprocess.py:93} INFO - 	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)
[2024-11-23T15:13:23.245+0000] {subprocess.py:93} INFO - 	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)
[2024-11-23T15:13:23.246+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:456)
[2024-11-23T15:13:23.247+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)
[2024-11-23T15:13:23.247+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)
[2024-11-23T15:13:23.248+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)
[2024-11-23T15:13:23.248+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:456)
[2024-11-23T15:13:23.249+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:588)
[2024-11-23T15:13:23.250+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:604)
[2024-11-23T15:13:23.250+0000] {subprocess.py:93} INFO - 	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)
[2024-11-23T15:13:23.251+0000] {subprocess.py:93} INFO - 	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
[2024-11-23T15:13:23.252+0000] {subprocess.py:93} INFO - 	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
[2024-11-23T15:13:23.253+0000] {subprocess.py:93} INFO - 	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
[2024-11-23T15:13:23.253+0000] {subprocess.py:93} INFO - 	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
[2024-11-23T15:13:23.254+0000] {subprocess.py:93} INFO - 	... 47 more
[2024-11-23T15:13:23.255+0000] {subprocess.py:93} INFO - 
[2024-11-23T15:13:24.466+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2024-11-23T15:13:24.544+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2024-11-23T15:13:24.571+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=transfer_to_bronze_layer_full, task_id=transfer_to_bronze_layer_full, execution_date=20241123T151127, start_date=20241123T151237, end_date=20241123T151324
[2024-11-23T15:13:24.835+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 323 for task transfer_to_bronze_layer_full (Bash command failed. The command returned a non-zero exit code 1.; 1469)
[2024-11-23T15:13:24.856+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-11-23T15:13:24.931+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
