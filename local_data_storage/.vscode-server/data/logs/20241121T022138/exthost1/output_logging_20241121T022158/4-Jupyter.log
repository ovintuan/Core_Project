Visual Studio Code (1.95.3, attached-container, desktop)
Jupyter Extension Version: 2024.10.0.
Python Extension Version: 2024.20.0.
Pylance Extension Version: 2024.11.2.
Platform: linux (x64).
Temp Storage folder ~/.vscode-server/data/User/globalStorage/ms-toolsai.jupyter/version-2024.10.0
Workspace folder /container/pyspark_workspace, Home = /root
02:24:10.952 [info] Starting Kernel (Python Path: /usr/local/bin/python, Unknown, 3.11.9) for '/container/pyspark_workspace/source_code/sample_code/Test_Project_1.ipynb' (disableUI=true)
02:24:12.995 [warn] Failed to get activated env vars for /usr/local/bin/python in 2047ms
02:24:13.022 [warn] Failed to get activated env vars for /usr/local/bin/python in 1674ms
02:24:13.065 [info] Process Execution: /usr/local/bin/python -c "import site;print("USER_BASE_VALUE");print(site.USER_BASE);print("USER_BASE_VALUE");"
02:24:14.036 [info] Process Execution: /usr/local/bin/python -m pip list
02:24:14.048 [info] Process Execution: /usr/local/bin/python -c "import ipykernel; print(ipykernel.__version__); print("5dc3a68c-e34e-4080-9c3e-2a532b2ccb4d"); print(ipykernel.__file__)"
02:24:14.130 [info] Process Execution: /usr/local/bin/python -m ipykernel_launcher --f=/~/.local/share/jupyter/runtime/kernel-v3f16be5c07b9bb7054087a71578e41092b97bc623.json
    > cwd: //container/pyspark_workspace/source_code/sample_code
02:24:25.596 [info] Kernel successfully started
02:24:25.633 [info] Process Execution: /usr/local/bin/python /~/.vscode-server/extensions/ms-toolsai.jupyter-2024.10.0-linux-x64/pythonFiles/printJupyterDataDir.py
02:26:10.213 [warn] Cell completed with errors iu [Error]: name 'Row' is not defined
    at n.execute (/~/.vscode-server/extensions/ms-toolsai.jupyter-2024.10.0-linux-x64/dist/extension.node.js:297:4958) {
  ename: 'NameError',
  evalue: "name 'Row' is not defined",
  traceback: [
    '\x1B[0;31m---------------------------------------------------------------------------\x1B[0m',
    '\x1B[0;31mNameError\x1B[0m                                 Traceback (most recent call last)',
    'Cell \x1B[0;32mIn[6], line 33\x1B[0m\n' +
      '\x1B[1;32m     31\x1B[0m customer_product_account_data, account_transaction_history_data \x1B[38;5;241m=\x1B[39m generator\x1B[38;5;241m.\x1B[39mgenerate_data(\x1B[38;5;241m20\x1B[39m)\n' +
      '\x1B[1;32m     32\x1B[0m \x1B[38;5;66;03m# Generate and insert data for CustomerProductAccount table\x1B[39;00m\n' +
      "\x1B[0;32m---> 33\x1B[0m \x1B[43minsert_data\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[38;5;124;43mCustomerProductAccount\x1B[39;49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mcustomer_product_account_data\x1B[49m\x1B[43m)\x1B[49m\n" +
      '\x1B[1;32m     34\x1B[0m \x1B[38;5;66;03m# Generate and insert data for AccountTransactionHistory table\x1B[39;00m\n' +
      "\x1B[1;32m     35\x1B[0m insert_data(\x1B[38;5;124m'\x1B[39m\x1B[38;5;124mAccountTransactionHistory\x1B[39m\x1B[38;5;124m'\x1B[39m, account_transaction_history_data)\n",
    'Cell \x1B[0;32mIn[6], line 18\x1B[0m, in \x1B[0;36minsert_data\x1B[0;34m(table_name, data)\x1B[0m\n' +
      '\x1B[1;32m     17\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21minsert_data\x1B[39m(table_name, data):\n' +
      '\x1B[0;32m---> 18\x1B[0m     df \x1B[38;5;241m=\x1B[39m spark\x1B[38;5;241m.\x1B[39mcreateDataFrame(\x1B[43m[\x1B[49m\x1B[43mRow\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43mi\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43;01mfor\x1B[39;49;00m\x1B[43m \x1B[49m\x1B[43mi\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;129;43;01min\x1B[39;49;00m\x1B[43m \x1B[49m\x1B[43mdata\x1B[49m\x1B[43m]\x1B[49m)\n' +
      '\x1B[1;32m     19\x1B[0m     df\x1B[38;5;241m.\x1B[39mwrite \\\n' +
      '\x1B[1;32m     20\x1B[0m         \x1B[38;5;241m.\x1B[39mformat(\x1B[38;5;124m"\x1B[39m\x1B[38;5;124mjdbc\x1B[39m\x1B[38;5;124m"\x1B[39m) \\\n' +
      '\x1B[1;32m     21\x1B[0m         \x1B[38;5;241m.\x1B[39moption(\x1B[38;5;124m"\x1B[39m\x1B[38;5;124murl\x1B[39m\x1B[38;5;124m"\x1B[39m, jdbc_url) \\\n' +
      '\x1B[0;32m   (...)\x1B[0m\n' +
      '\x1B[1;32m     25\x1B[0m         \x1B[38;5;241m.\x1B[39mmode(\x1B[38;5;124m"\x1B[39m\x1B[38;5;124mappend\x1B[39m\x1B[38;5;124m"\x1B[39m) \\\n' +
      '\x1B[1;32m     26\x1B[0m         \x1B[38;5;241m.\x1B[39msave()\n',
    'Cell \x1B[0;32mIn[6], line 18\x1B[0m, in \x1B[0;36m<listcomp>\x1B[0;34m(.0)\x1B[0m\n' +
      '\x1B[1;32m     17\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21minsert_data\x1B[39m(table_name, data):\n' +
      '\x1B[0;32m---> 18\x1B[0m     df \x1B[38;5;241m=\x1B[39m spark\x1B[38;5;241m.\x1B[39mcreateDataFrame([\x1B[43mRow\x1B[49m(\x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mi) \x1B[38;5;28;01mfor\x1B[39;00m i \x1B[38;5;129;01min\x1B[39;00m data])\n' +
      '\x1B[1;32m     19\x1B[0m     df\x1B[38;5;241m.\x1B[39mwrite \\\n' +
      '\x1B[1;32m     20\x1B[0m         \x1B[38;5;241m.\x1B[39mformat(\x1B[38;5;124m"\x1B[39m\x1B[38;5;124mjdbc\x1B[39m\x1B[38;5;124m"\x1B[39m) \\\n' +
      '\x1B[1;32m     21\x1B[0m         \x1B[38;5;241m.\x1B[39moption(\x1B[38;5;124m"\x1B[39m\x1B[38;5;124murl\x1B[39m\x1B[38;5;124m"\x1B[39m, jdbc_url) \\\n' +
      '\x1B[0;32m   (...)\x1B[0m\n' +
      '\x1B[1;32m     25\x1B[0m         \x1B[38;5;241m.\x1B[39mmode(\x1B[38;5;124m"\x1B[39m\x1B[38;5;124mappend\x1B[39m\x1B[38;5;124m"\x1B[39m) \\\n' +
      '\x1B[1;32m     26\x1B[0m         \x1B[38;5;241m.\x1B[39msave()\n',
    "\x1B[0;31mNameError\x1B[0m: name 'Row' is not defined"
  ]
}
02:26:48.686 [warn] Cell completed with errors iu [Error]: An error occurred while calling o65.save.
: java.sql.SQLException: No suitable driver
	at java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)

    at n.execute (/~/.vscode-server/extensions/ms-toolsai.jupyter-2024.10.0-linux-x64/dist/extension.node.js:297:4958) {
  ename: 'Py4JJavaError',
  evalue: 'An error occurred while calling o65.save.\n' +
    ': java.sql.SQLException: No suitable driver\n' +
    '\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\n' +
    '\tat scala.Option.getOrElse(Option.scala:189)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
    '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
    '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
    '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
    '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
    '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
    '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
    '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
    '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
    '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
    '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
    '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
    '\tat java.base/java.lang.Thread.run(Thread.java:840)\n',
  traceback: [
    '\x1B[0;31m---------------------------------------------------------------------------\x1B[0m',
    '\x1B[0;31mPy4JJavaError\x1B[0m                             Traceback (most recent call last)',
    'Cell \x1B[0;32mIn[8], line 33\x1B[0m\n' +
      '\x1B[1;32m     31\x1B[0m customer_product_account_data, account_transaction_history_data \x1B[38;5;241m=\x1B[39m generator\x1B[38;5;241m.\x1B[39mgenerate_data(\x1B[38;5;241m20\x1B[39m)\n' +
      '\x1B[1;32m     32\x1B[0m \x1B[38;5;66;03m# Generate and insert data for CustomerProductAccount table\x1B[39;00m\n' +
      "\x1B[0;32m---> 33\x1B[0m \x1B[43minsert_data\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[38;5;124;43mCustomerProductAccount\x1B[39;49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mcustomer_product_account_data\x1B[49m\x1B[43m)\x1B[49m\n" +
      '\x1B[1;32m     34\x1B[0m \x1B[38;5;66;03m# Generate and insert data for AccountTransactionHistory table\x1B[39;00m\n' +
      "\x1B[1;32m     35\x1B[0m insert_data(\x1B[38;5;124m'\x1B[39m\x1B[38;5;124mAccountTransactionHistory\x1B[39m\x1B[38;5;124m'\x1B[39m, account_transaction_history_data)\n",
    'Cell \x1B[0;32mIn[8], line 26\x1B[0m, in \x1B[0;36minsert_data\x1B[0;34m(table_name, data)\x1B[0m\n' +
      '\x1B[1;32m     17\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21minsert_data\x1B[39m(table_name, data):\n' +
      '\x1B[1;32m     18\x1B[0m     df \x1B[38;5;241m=\x1B[39m spark\x1B[38;5;241m.\x1B[39mcreateDataFrame([Row(\x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mi) \x1B[38;5;28;01mfor\x1B[39;00m i \x1B[38;5;129;01min\x1B[39;00m data])\n' +
      '\x1B[1;32m     19\x1B[0m     \x1B[43mdf\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mwrite\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     20\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mformat\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mjdbc\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     21\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43murl\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mjdbc_url\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     22\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdbtable\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mtable_name\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     23\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43muser\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43musername\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     24\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mpassword\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mpassword\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     25\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mmode\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mappend\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[0;32m---> 26\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1461\x1B[0m, in \x1B[0;36mDataFrameWriter.save\x1B[0;34m(self, path, format, mode, partitionBy, **options)\x1B[0m\n' +
      '\x1B[1;32m   1459\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mformat(\x1B[38;5;28mformat\x1B[39m)\n' +
      '\x1B[1;32m   1460\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m path \x1B[38;5;129;01mis\x1B[39;00m \x1B[38;5;28;01mNone\x1B[39;00m:\n' +
      '\x1B[0;32m-> 1461\x1B[0m     \x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43m_jwrite\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1462\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m   1463\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39m_jwrite\x1B[38;5;241m.\x1B[39msave(path)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\x1B[0m, in \x1B[0;36mJavaMember.__call__\x1B[0;34m(self, *args)\x1B[0m\n' +
      '\x1B[1;32m   1316\x1B[0m command \x1B[38;5;241m=\x1B[39m proto\x1B[38;5;241m.\x1B[39mCALL_COMMAND_NAME \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1317\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mcommand_header \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1318\x1B[0m     args_command \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1319\x1B[0m     proto\x1B[38;5;241m.\x1B[39mEND_COMMAND_PART\n' +
      '\x1B[1;32m   1321\x1B[0m answer \x1B[38;5;241m=\x1B[39m \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mgateway_client\x1B[38;5;241m.\x1B[39msend_command(command)\n' +
      '\x1B[0;32m-> 1322\x1B[0m return_value \x1B[38;5;241m=\x1B[39m \x1B[43mget_return_value\x1B[49m\x1B[43m(\x1B[49m\n' +
      '\x1B[1;32m   1323\x1B[0m \x1B[43m    \x1B[49m\x1B[43manswer\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mgateway_client\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mtarget_id\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mname\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1325\x1B[0m \x1B[38;5;28;01mfor\x1B[39;00m temp_arg \x1B[38;5;129;01min\x1B[39;00m temp_args:\n' +
      '\x1B[1;32m   1326\x1B[0m     \x1B[38;5;28;01mif\x1B[39;00m \x1B[38;5;28mhasattr\x1B[39m(temp_arg, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m_detach\x1B[39m\x1B[38;5;124m"\x1B[39m):\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\x1B[0m, in \x1B[0;36mcapture_sql_exception.<locals>.deco\x1B[0;34m(*a, **kw)\x1B[0m\n' +
      '\x1B[1;32m    177\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21mdeco\x1B[39m(\x1B[38;5;241m*\x1B[39ma: Any, \x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mkw: Any) \x1B[38;5;241m-\x1B[39m\x1B[38;5;241m>\x1B[39m Any:\n' +
      '\x1B[1;32m    178\x1B[0m     \x1B[38;5;28;01mtry\x1B[39;00m:\n' +
      '\x1B[0;32m--> 179\x1B[0m         \x1B[38;5;28;01mreturn\x1B[39;00m \x1B[43mf\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43ma\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43mkw\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m    180\x1B[0m     \x1B[38;5;28;01mexcept\x1B[39;00m Py4JJavaError \x1B[38;5;28;01mas\x1B[39;00m e:\n' +
      '\x1B[1;32m    181\x1B[0m         converted \x1B[38;5;241m=\x1B[39m convert_exception(e\x1B[38;5;241m.\x1B[39mjava_exception)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\x1B[0m, in \x1B[0;36mget_return_value\x1B[0;34m(answer, gateway_client, target_id, name)\x1B[0m\n' +
      '\x1B[1;32m    324\x1B[0m value \x1B[38;5;241m=\x1B[39m OUTPUT_CONVERTER[\x1B[38;5;28mtype\x1B[39m](answer[\x1B[38;5;241m2\x1B[39m:], gateway_client)\n' +
      '\x1B[1;32m    325\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m answer[\x1B[38;5;241m1\x1B[39m] \x1B[38;5;241m==\x1B[39m REFERENCE_TYPE:\n' +
      '\x1B[0;32m--> 326\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JJavaError(\n' +
      '\x1B[1;32m    327\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m.\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    328\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name), value)\n' +
      '\x1B[1;32m    329\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m    330\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JError(\n' +
      '\x1B[1;32m    331\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m. Trace:\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;132;01m{3}\x1B[39;00m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    332\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name, value))\n',
    '\x1B[0;31mPy4JJavaError\x1B[0m: An error occurred while calling o65.save.\n' +
      ': java.sql.SQLException: No suitable driver\n' +
      '\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\n' +
      '\tat scala.Option.getOrElse(Option.scala:189)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
      '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
      '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
      '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
      '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
      '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
      '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
      '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
      '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
      '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
      '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
      '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
      '\tat java.base/java.lang.Thread.run(Thread.java:840)\n'
  ]
}
02:29:36.696 [warn] Cell completed with errors iu [Error]: An error occurred while calling o92.save.
: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)

    at n.execute (/~/.vscode-server/extensions/ms-toolsai.jupyter-2024.10.0-linux-x64/dist/extension.node.js:297:4958) {
  ename: 'Py4JJavaError',
  evalue: 'An error occurred while calling o92.save.\n' +
    ': java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\n' +
    '\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n' +
    '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n' +
    '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n' +
    '\tat scala.Option.foreach(Option.scala:407)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
    '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
    '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
    '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
    '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
    '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
    '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
    '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
    '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
    '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
    '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
    '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
    '\tat java.base/java.lang.Thread.run(Thread.java:840)\n',
  traceback: [
    '\x1B[0;31m---------------------------------------------------------------------------\x1B[0m',
    '\x1B[0;31mPy4JJavaError\x1B[0m                             Traceback (most recent call last)',
    'Cell \x1B[0;32mIn[9], line 34\x1B[0m\n' +
      '\x1B[1;32m     32\x1B[0m customer_product_account_data, account_transaction_history_data \x1B[38;5;241m=\x1B[39m generator\x1B[38;5;241m.\x1B[39mgenerate_data(\x1B[38;5;241m20\x1B[39m)\n' +
      '\x1B[1;32m     33\x1B[0m \x1B[38;5;66;03m# Generate and insert data for CustomerProductAccount table\x1B[39;00m\n' +
      "\x1B[0;32m---> 34\x1B[0m \x1B[43minsert_data\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[38;5;124;43mCustomerProductAccount\x1B[39;49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mcustomer_product_account_data\x1B[49m\x1B[43m)\x1B[49m\n" +
      '\x1B[1;32m     35\x1B[0m \x1B[38;5;66;03m# Generate and insert data for AccountTransactionHistory table\x1B[39;00m\n' +
      "\x1B[1;32m     36\x1B[0m insert_data(\x1B[38;5;124m'\x1B[39m\x1B[38;5;124mAccountTransactionHistory\x1B[39m\x1B[38;5;124m'\x1B[39m, account_transaction_history_data)\n",
    'Cell \x1B[0;32mIn[9], line 27\x1B[0m, in \x1B[0;36minsert_data\x1B[0;34m(table_name, data)\x1B[0m\n' +
      '\x1B[1;32m     17\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21minsert_data\x1B[39m(table_name, data):\n' +
      '\x1B[1;32m     18\x1B[0m     df \x1B[38;5;241m=\x1B[39m spark\x1B[38;5;241m.\x1B[39mcreateDataFrame([Row(\x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mi) \x1B[38;5;28;01mfor\x1B[39;00m i \x1B[38;5;129;01min\x1B[39;00m data])\n' +
      '\x1B[1;32m     19\x1B[0m     \x1B[43mdf\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mwrite\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     20\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mformat\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mjdbc\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     21\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdriver\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mcom.microsoft.sqlserver.jdbc.SQLServerDriver\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     22\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43murl\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mjdbc_url\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     23\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdbtable\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mtable_name\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     24\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43muser\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43musername\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     25\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mpassword\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mpassword\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     26\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mmode\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mappend\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[0;32m---> 27\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1461\x1B[0m, in \x1B[0;36mDataFrameWriter.save\x1B[0;34m(self, path, format, mode, partitionBy, **options)\x1B[0m\n' +
      '\x1B[1;32m   1459\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mformat(\x1B[38;5;28mformat\x1B[39m)\n' +
      '\x1B[1;32m   1460\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m path \x1B[38;5;129;01mis\x1B[39;00m \x1B[38;5;28;01mNone\x1B[39;00m:\n' +
      '\x1B[0;32m-> 1461\x1B[0m     \x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43m_jwrite\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1462\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m   1463\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39m_jwrite\x1B[38;5;241m.\x1B[39msave(path)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\x1B[0m, in \x1B[0;36mJavaMember.__call__\x1B[0;34m(self, *args)\x1B[0m\n' +
      '\x1B[1;32m   1316\x1B[0m command \x1B[38;5;241m=\x1B[39m proto\x1B[38;5;241m.\x1B[39mCALL_COMMAND_NAME \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1317\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mcommand_header \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1318\x1B[0m     args_command \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1319\x1B[0m     proto\x1B[38;5;241m.\x1B[39mEND_COMMAND_PART\n' +
      '\x1B[1;32m   1321\x1B[0m answer \x1B[38;5;241m=\x1B[39m \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mgateway_client\x1B[38;5;241m.\x1B[39msend_command(command)\n' +
      '\x1B[0;32m-> 1322\x1B[0m return_value \x1B[38;5;241m=\x1B[39m \x1B[43mget_return_value\x1B[49m\x1B[43m(\x1B[49m\n' +
      '\x1B[1;32m   1323\x1B[0m \x1B[43m    \x1B[49m\x1B[43manswer\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mgateway_client\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mtarget_id\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mname\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1325\x1B[0m \x1B[38;5;28;01mfor\x1B[39;00m temp_arg \x1B[38;5;129;01min\x1B[39;00m temp_args:\n' +
      '\x1B[1;32m   1326\x1B[0m     \x1B[38;5;28;01mif\x1B[39;00m \x1B[38;5;28mhasattr\x1B[39m(temp_arg, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m_detach\x1B[39m\x1B[38;5;124m"\x1B[39m):\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\x1B[0m, in \x1B[0;36mcapture_sql_exception.<locals>.deco\x1B[0;34m(*a, **kw)\x1B[0m\n' +
      '\x1B[1;32m    177\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21mdeco\x1B[39m(\x1B[38;5;241m*\x1B[39ma: Any, \x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mkw: Any) \x1B[38;5;241m-\x1B[39m\x1B[38;5;241m>\x1B[39m Any:\n' +
      '\x1B[1;32m    178\x1B[0m     \x1B[38;5;28;01mtry\x1B[39;00m:\n' +
      '\x1B[0;32m--> 179\x1B[0m         \x1B[38;5;28;01mreturn\x1B[39;00m \x1B[43mf\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43ma\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43mkw\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m    180\x1B[0m     \x1B[38;5;28;01mexcept\x1B[39;00m Py4JJavaError \x1B[38;5;28;01mas\x1B[39;00m e:\n' +
      '\x1B[1;32m    181\x1B[0m         converted \x1B[38;5;241m=\x1B[39m convert_exception(e\x1B[38;5;241m.\x1B[39mjava_exception)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\x1B[0m, in \x1B[0;36mget_return_value\x1B[0;34m(answer, gateway_client, target_id, name)\x1B[0m\n' +
      '\x1B[1;32m    324\x1B[0m value \x1B[38;5;241m=\x1B[39m OUTPUT_CONVERTER[\x1B[38;5;28mtype\x1B[39m](answer[\x1B[38;5;241m2\x1B[39m:], gateway_client)\n' +
      '\x1B[1;32m    325\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m answer[\x1B[38;5;241m1\x1B[39m] \x1B[38;5;241m==\x1B[39m REFERENCE_TYPE:\n' +
      '\x1B[0;32m--> 326\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JJavaError(\n' +
      '\x1B[1;32m    327\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m.\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    328\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name), value)\n' +
      '\x1B[1;32m    329\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m    330\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JError(\n' +
      '\x1B[1;32m    331\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m. Trace:\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;132;01m{3}\x1B[39;00m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    332\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name, value))\n',
    '\x1B[0;31mPy4JJavaError\x1B[0m: An error occurred while calling o92.save.\n' +
      ': java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\n' +
      '\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n' +
      '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n' +
      '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n' +
      '\tat scala.Option.foreach(Option.scala:407)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
      '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
      '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
      '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
      '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
      '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
      '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
      '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
      '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
      '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
      '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
      '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
      '\tat java.base/java.lang.Thread.run(Thread.java:840)\n'
  ]
}
02:35:35.370 [info] Restart requested /container/pyspark_workspace/source_code/sample_code/Test_Project_1.ipynb
02:35:36.975 [warn] Disposing old controller startUsingPythonInterpreter:'.jvsc74a57bd06699b966c21a81be21759e40053de52c009daad87cbe5a3dc643468ce4ab3367.d:\Program Files\Python\python.exe.d:\Program Files\Python\python.exe.-m#ipykernel_launcher' for view = 'jupyter-notebook'
02:35:36.984 [warn] Disposing old controller startUsingPythonInterpreter:'.jvsc74a57bd06699b966c21a81be21759e40053de52c009daad87cbe5a3dc643468ce4ab3367.d:\Program Files\Python\python.exe.d:\Program Files\Python\python.exe.-m#ipykernel_launcher (Interactive)' for view = 'interactive'
02:35:37.212 [warn] Failed to get activated env vars for /usr/local/bin/python in 1541ms
02:35:37.228 [info] Process Execution: /usr/local/bin/python -c "import ipykernel; print(ipykernel.__version__); print("5dc3a68c-e34e-4080-9c3e-2a532b2ccb4d"); print(ipykernel.__file__)"
02:35:37.635 [warn] Failed to get activated env vars for /usr/local/bin/python in 280ms
02:35:37.655 [info] Process Execution: /usr/local/bin/python -m ipykernel_launcher --f=/~/.local/share/jupyter/runtime/kernel-v378c476e200e173ac370f28ae0e1e3e637c308d89.json
    > cwd: //container/pyspark_workspace/source_code/sample_code
02:35:41.845 [info] Restarted 6c8c97de-d785-4080-b9e4-55baf67ded63
02:36:06.873 [info] Disposing request as the cell (-1) was deleted /container/pyspark_workspace/source_code/sample_code/Test_Project_1.ipynb
02:37:34.665 [warn] Cell completed with errors iu [Error]: An error occurred while calling o66.save.
: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)

    at n.execute (/~/.vscode-server/extensions/ms-toolsai.jupyter-2024.10.0-linux-x64/dist/extension.node.js:297:4958) {
  ename: 'Py4JJavaError',
  evalue: 'An error occurred while calling o66.save.\n' +
    ': java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\n' +
    '\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n' +
    '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n' +
    '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n' +
    '\tat scala.Option.foreach(Option.scala:407)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
    '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
    '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
    '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
    '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
    '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
    '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
    '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
    '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
    '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
    '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
    '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
    '\tat java.base/java.lang.Thread.run(Thread.java:840)\n',
  traceback: [
    '\x1B[0;31m---------------------------------------------------------------------------\x1B[0m',
    '\x1B[0;31mPy4JJavaError\x1B[0m                             Traceback (most recent call last)',
    'Cell \x1B[0;32mIn[9], line 34\x1B[0m\n' +
      '\x1B[1;32m     32\x1B[0m customer_product_account_data, account_transaction_history_data \x1B[38;5;241m=\x1B[39m generator\x1B[38;5;241m.\x1B[39mgenerate_data(\x1B[38;5;241m20\x1B[39m)\n' +
      '\x1B[1;32m     33\x1B[0m \x1B[38;5;66;03m# Generate and insert data for CustomerProductAccount table\x1B[39;00m\n' +
      "\x1B[0;32m---> 34\x1B[0m \x1B[43minsert_data\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[38;5;124;43mCustomerProductAccount\x1B[39;49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mcustomer_product_account_data\x1B[49m\x1B[43m)\x1B[49m\n" +
      '\x1B[1;32m     35\x1B[0m \x1B[38;5;66;03m# Generate and insert data for AccountTransactionHistory table\x1B[39;00m\n' +
      "\x1B[1;32m     36\x1B[0m insert_data(\x1B[38;5;124m'\x1B[39m\x1B[38;5;124mAccountTransactionHistory\x1B[39m\x1B[38;5;124m'\x1B[39m, account_transaction_history_data)\n",
    'Cell \x1B[0;32mIn[9], line 27\x1B[0m, in \x1B[0;36minsert_data\x1B[0;34m(table_name, data)\x1B[0m\n' +
      '\x1B[1;32m     17\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21minsert_data\x1B[39m(table_name, data):\n' +
      '\x1B[1;32m     18\x1B[0m     df \x1B[38;5;241m=\x1B[39m spark\x1B[38;5;241m.\x1B[39mcreateDataFrame([Row(\x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mi) \x1B[38;5;28;01mfor\x1B[39;00m i \x1B[38;5;129;01min\x1B[39;00m data])\n' +
      '\x1B[1;32m     19\x1B[0m     \x1B[43mdf\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mwrite\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     20\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mformat\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mjdbc\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     21\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdriver\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mcom.microsoft.sqlserver.jdbc.SQLServerDriver\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     22\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43murl\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mjdbc_url\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     23\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdbtable\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mtable_name\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     24\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43muser\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43musername\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     25\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mpassword\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mpassword\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     26\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mmode\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mappend\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[0;32m---> 27\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1461\x1B[0m, in \x1B[0;36mDataFrameWriter.save\x1B[0;34m(self, path, format, mode, partitionBy, **options)\x1B[0m\n' +
      '\x1B[1;32m   1459\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mformat(\x1B[38;5;28mformat\x1B[39m)\n' +
      '\x1B[1;32m   1460\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m path \x1B[38;5;129;01mis\x1B[39;00m \x1B[38;5;28;01mNone\x1B[39;00m:\n' +
      '\x1B[0;32m-> 1461\x1B[0m     \x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43m_jwrite\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1462\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m   1463\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39m_jwrite\x1B[38;5;241m.\x1B[39msave(path)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\x1B[0m, in \x1B[0;36mJavaMember.__call__\x1B[0;34m(self, *args)\x1B[0m\n' +
      '\x1B[1;32m   1316\x1B[0m command \x1B[38;5;241m=\x1B[39m proto\x1B[38;5;241m.\x1B[39mCALL_COMMAND_NAME \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1317\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mcommand_header \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1318\x1B[0m     args_command \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1319\x1B[0m     proto\x1B[38;5;241m.\x1B[39mEND_COMMAND_PART\n' +
      '\x1B[1;32m   1321\x1B[0m answer \x1B[38;5;241m=\x1B[39m \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mgateway_client\x1B[38;5;241m.\x1B[39msend_command(command)\n' +
      '\x1B[0;32m-> 1322\x1B[0m return_value \x1B[38;5;241m=\x1B[39m \x1B[43mget_return_value\x1B[49m\x1B[43m(\x1B[49m\n' +
      '\x1B[1;32m   1323\x1B[0m \x1B[43m    \x1B[49m\x1B[43manswer\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mgateway_client\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mtarget_id\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mname\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1325\x1B[0m \x1B[38;5;28;01mfor\x1B[39;00m temp_arg \x1B[38;5;129;01min\x1B[39;00m temp_args:\n' +
      '\x1B[1;32m   1326\x1B[0m     \x1B[38;5;28;01mif\x1B[39;00m \x1B[38;5;28mhasattr\x1B[39m(temp_arg, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m_detach\x1B[39m\x1B[38;5;124m"\x1B[39m):\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\x1B[0m, in \x1B[0;36mcapture_sql_exception.<locals>.deco\x1B[0;34m(*a, **kw)\x1B[0m\n' +
      '\x1B[1;32m    177\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21mdeco\x1B[39m(\x1B[38;5;241m*\x1B[39ma: Any, \x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mkw: Any) \x1B[38;5;241m-\x1B[39m\x1B[38;5;241m>\x1B[39m Any:\n' +
      '\x1B[1;32m    178\x1B[0m     \x1B[38;5;28;01mtry\x1B[39;00m:\n' +
      '\x1B[0;32m--> 179\x1B[0m         \x1B[38;5;28;01mreturn\x1B[39;00m \x1B[43mf\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43ma\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43mkw\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m    180\x1B[0m     \x1B[38;5;28;01mexcept\x1B[39;00m Py4JJavaError \x1B[38;5;28;01mas\x1B[39;00m e:\n' +
      '\x1B[1;32m    181\x1B[0m         converted \x1B[38;5;241m=\x1B[39m convert_exception(e\x1B[38;5;241m.\x1B[39mjava_exception)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\x1B[0m, in \x1B[0;36mget_return_value\x1B[0;34m(answer, gateway_client, target_id, name)\x1B[0m\n' +
      '\x1B[1;32m    324\x1B[0m value \x1B[38;5;241m=\x1B[39m OUTPUT_CONVERTER[\x1B[38;5;28mtype\x1B[39m](answer[\x1B[38;5;241m2\x1B[39m:], gateway_client)\n' +
      '\x1B[1;32m    325\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m answer[\x1B[38;5;241m1\x1B[39m] \x1B[38;5;241m==\x1B[39m REFERENCE_TYPE:\n' +
      '\x1B[0;32m--> 326\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JJavaError(\n' +
      '\x1B[1;32m    327\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m.\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    328\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name), value)\n' +
      '\x1B[1;32m    329\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m    330\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JError(\n' +
      '\x1B[1;32m    331\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m. Trace:\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;132;01m{3}\x1B[39;00m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    332\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name, value))\n',
    '\x1B[0;31mPy4JJavaError\x1B[0m: An error occurred while calling o66.save.\n' +
      ': java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\n' +
      '\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n' +
      '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n' +
      '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n' +
      '\tat scala.Option.foreach(Option.scala:407)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
      '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
      '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
      '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
      '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
      '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
      '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
      '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
      '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
      '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
      '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
      '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
      '\tat java.base/java.lang.Thread.run(Thread.java:840)\n'
  ]
}
02:38:04.295 [warn] Cell completed with errors iu [Error]: An error occurred while calling o92.save.
: java.sql.SQLException: No suitable driver
	at java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)

    at n.execute (/~/.vscode-server/extensions/ms-toolsai.jupyter-2024.10.0-linux-x64/dist/extension.node.js:297:4958) {
  ename: 'Py4JJavaError',
  evalue: 'An error occurred while calling o92.save.\n' +
    ': java.sql.SQLException: No suitable driver\n' +
    '\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\n' +
    '\tat scala.Option.getOrElse(Option.scala:189)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
    '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
    '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
    '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
    '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
    '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
    '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
    '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
    '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
    '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
    '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
    '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
    '\tat java.base/java.lang.Thread.run(Thread.java:840)\n',
  traceback: [
    '\x1B[0;31m---------------------------------------------------------------------------\x1B[0m',
    '\x1B[0;31mPy4JJavaError\x1B[0m                             Traceback (most recent call last)',
    'Cell \x1B[0;32mIn[10], line 33\x1B[0m\n' +
      '\x1B[1;32m     31\x1B[0m customer_product_account_data, account_transaction_history_data \x1B[38;5;241m=\x1B[39m generator\x1B[38;5;241m.\x1B[39mgenerate_data(\x1B[38;5;241m20\x1B[39m)\n' +
      '\x1B[1;32m     32\x1B[0m \x1B[38;5;66;03m# Generate and insert data for CustomerProductAccount table\x1B[39;00m\n' +
      "\x1B[0;32m---> 33\x1B[0m \x1B[43minsert_data\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[38;5;124;43mCustomerProductAccount\x1B[39;49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mcustomer_product_account_data\x1B[49m\x1B[43m)\x1B[49m\n" +
      '\x1B[1;32m     34\x1B[0m \x1B[38;5;66;03m# Generate and insert data for AccountTransactionHistory table\x1B[39;00m\n' +
      "\x1B[1;32m     35\x1B[0m insert_data(\x1B[38;5;124m'\x1B[39m\x1B[38;5;124mAccountTransactionHistory\x1B[39m\x1B[38;5;124m'\x1B[39m, account_transaction_history_data)\n",
    'Cell \x1B[0;32mIn[10], line 26\x1B[0m, in \x1B[0;36minsert_data\x1B[0;34m(table_name, data)\x1B[0m\n' +
      '\x1B[1;32m     17\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21minsert_data\x1B[39m(table_name, data):\n' +
      '\x1B[1;32m     18\x1B[0m     df \x1B[38;5;241m=\x1B[39m spark\x1B[38;5;241m.\x1B[39mcreateDataFrame([Row(\x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mi) \x1B[38;5;28;01mfor\x1B[39;00m i \x1B[38;5;129;01min\x1B[39;00m data])\n' +
      '\x1B[1;32m     19\x1B[0m     \x1B[43mdf\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mwrite\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     20\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mformat\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mjdbc\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     21\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43murl\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mjdbc_url\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     22\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdbtable\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mtable_name\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     23\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43muser\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43musername\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     24\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mpassword\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mpassword\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     25\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mmode\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mappend\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[0;32m---> 26\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1461\x1B[0m, in \x1B[0;36mDataFrameWriter.save\x1B[0;34m(self, path, format, mode, partitionBy, **options)\x1B[0m\n' +
      '\x1B[1;32m   1459\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mformat(\x1B[38;5;28mformat\x1B[39m)\n' +
      '\x1B[1;32m   1460\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m path \x1B[38;5;129;01mis\x1B[39;00m \x1B[38;5;28;01mNone\x1B[39;00m:\n' +
      '\x1B[0;32m-> 1461\x1B[0m     \x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43m_jwrite\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1462\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m   1463\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39m_jwrite\x1B[38;5;241m.\x1B[39msave(path)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\x1B[0m, in \x1B[0;36mJavaMember.__call__\x1B[0;34m(self, *args)\x1B[0m\n' +
      '\x1B[1;32m   1316\x1B[0m command \x1B[38;5;241m=\x1B[39m proto\x1B[38;5;241m.\x1B[39mCALL_COMMAND_NAME \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1317\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mcommand_header \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1318\x1B[0m     args_command \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1319\x1B[0m     proto\x1B[38;5;241m.\x1B[39mEND_COMMAND_PART\n' +
      '\x1B[1;32m   1321\x1B[0m answer \x1B[38;5;241m=\x1B[39m \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mgateway_client\x1B[38;5;241m.\x1B[39msend_command(command)\n' +
      '\x1B[0;32m-> 1322\x1B[0m return_value \x1B[38;5;241m=\x1B[39m \x1B[43mget_return_value\x1B[49m\x1B[43m(\x1B[49m\n' +
      '\x1B[1;32m   1323\x1B[0m \x1B[43m    \x1B[49m\x1B[43manswer\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mgateway_client\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mtarget_id\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mname\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1325\x1B[0m \x1B[38;5;28;01mfor\x1B[39;00m temp_arg \x1B[38;5;129;01min\x1B[39;00m temp_args:\n' +
      '\x1B[1;32m   1326\x1B[0m     \x1B[38;5;28;01mif\x1B[39;00m \x1B[38;5;28mhasattr\x1B[39m(temp_arg, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m_detach\x1B[39m\x1B[38;5;124m"\x1B[39m):\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\x1B[0m, in \x1B[0;36mcapture_sql_exception.<locals>.deco\x1B[0;34m(*a, **kw)\x1B[0m\n' +
      '\x1B[1;32m    177\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21mdeco\x1B[39m(\x1B[38;5;241m*\x1B[39ma: Any, \x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mkw: Any) \x1B[38;5;241m-\x1B[39m\x1B[38;5;241m>\x1B[39m Any:\n' +
      '\x1B[1;32m    178\x1B[0m     \x1B[38;5;28;01mtry\x1B[39;00m:\n' +
      '\x1B[0;32m--> 179\x1B[0m         \x1B[38;5;28;01mreturn\x1B[39;00m \x1B[43mf\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43ma\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43mkw\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m    180\x1B[0m     \x1B[38;5;28;01mexcept\x1B[39;00m Py4JJavaError \x1B[38;5;28;01mas\x1B[39;00m e:\n' +
      '\x1B[1;32m    181\x1B[0m         converted \x1B[38;5;241m=\x1B[39m convert_exception(e\x1B[38;5;241m.\x1B[39mjava_exception)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\x1B[0m, in \x1B[0;36mget_return_value\x1B[0;34m(answer, gateway_client, target_id, name)\x1B[0m\n' +
      '\x1B[1;32m    324\x1B[0m value \x1B[38;5;241m=\x1B[39m OUTPUT_CONVERTER[\x1B[38;5;28mtype\x1B[39m](answer[\x1B[38;5;241m2\x1B[39m:], gateway_client)\n' +
      '\x1B[1;32m    325\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m answer[\x1B[38;5;241m1\x1B[39m] \x1B[38;5;241m==\x1B[39m REFERENCE_TYPE:\n' +
      '\x1B[0;32m--> 326\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JJavaError(\n' +
      '\x1B[1;32m    327\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m.\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    328\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name), value)\n' +
      '\x1B[1;32m    329\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m    330\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JError(\n' +
      '\x1B[1;32m    331\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m. Trace:\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;132;01m{3}\x1B[39;00m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    332\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name, value))\n',
    '\x1B[0;31mPy4JJavaError\x1B[0m: An error occurred while calling o92.save.\n' +
      ': java.sql.SQLException: No suitable driver\n' +
      '\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\n' +
      '\tat scala.Option.getOrElse(Option.scala:189)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
      '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
      '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
      '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
      '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
      '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
      '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
      '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
      '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
      '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
      '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
      '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
      '\tat java.base/java.lang.Thread.run(Thread.java:840)\n'
  ]
}
02:38:46.349 [warn] Cell completed with errors iu [Error]: An error occurred while calling o119.save.
: java.lang.ClassNotFoundException:  com.microsoft.sqlserver
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)

    at n.execute (/~/.vscode-server/extensions/ms-toolsai.jupyter-2024.10.0-linux-x64/dist/extension.node.js:297:4958) {
  ename: 'Py4JJavaError',
  evalue: 'An error occurred while calling o119.save.\n' +
    ': java.lang.ClassNotFoundException:  com.microsoft.sqlserver\n' +
    '\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n' +
    '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n' +
    '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n' +
    '\tat scala.Option.foreach(Option.scala:407)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
    '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
    '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
    '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
    '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
    '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
    '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
    '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
    '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
    '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
    '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
    '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
    '\tat java.base/java.lang.Thread.run(Thread.java:840)\n',
  traceback: [
    '\x1B[0;31m---------------------------------------------------------------------------\x1B[0m',
    '\x1B[0;31mPy4JJavaError\x1B[0m                             Traceback (most recent call last)',
    'Cell \x1B[0;32mIn[11], line 34\x1B[0m\n' +
      '\x1B[1;32m     32\x1B[0m customer_product_account_data, account_transaction_history_data \x1B[38;5;241m=\x1B[39m generator\x1B[38;5;241m.\x1B[39mgenerate_data(\x1B[38;5;241m20\x1B[39m)\n' +
      '\x1B[1;32m     33\x1B[0m \x1B[38;5;66;03m# Generate and insert data for CustomerProductAccount table\x1B[39;00m\n' +
      "\x1B[0;32m---> 34\x1B[0m \x1B[43minsert_data\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[38;5;124;43mCustomerProductAccount\x1B[39;49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mcustomer_product_account_data\x1B[49m\x1B[43m)\x1B[49m\n" +
      '\x1B[1;32m     35\x1B[0m \x1B[38;5;66;03m# Generate and insert data for AccountTransactionHistory table\x1B[39;00m\n' +
      "\x1B[1;32m     36\x1B[0m insert_data(\x1B[38;5;124m'\x1B[39m\x1B[38;5;124mAccountTransactionHistory\x1B[39m\x1B[38;5;124m'\x1B[39m, account_transaction_history_data)\n",
    'Cell \x1B[0;32mIn[11], line 27\x1B[0m, in \x1B[0;36minsert_data\x1B[0;34m(table_name, data)\x1B[0m\n' +
      '\x1B[1;32m     17\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21minsert_data\x1B[39m(table_name, data):\n' +
      '\x1B[1;32m     18\x1B[0m     df \x1B[38;5;241m=\x1B[39m spark\x1B[38;5;241m.\x1B[39mcreateDataFrame([Row(\x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mi) \x1B[38;5;28;01mfor\x1B[39;00m i \x1B[38;5;129;01min\x1B[39;00m data])\n' +
      '\x1B[1;32m     19\x1B[0m     \x1B[43mdf\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mwrite\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     20\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mformat\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mjdbc\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     21\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdriver\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43m com.microsoft.sqlserver\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     22\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43murl\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mjdbc_url\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     23\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdbtable\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mtable_name\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     24\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43muser\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43musername\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     25\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mpassword\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mpassword\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     26\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mmode\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mappend\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[0;32m---> 27\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1461\x1B[0m, in \x1B[0;36mDataFrameWriter.save\x1B[0;34m(self, path, format, mode, partitionBy, **options)\x1B[0m\n' +
      '\x1B[1;32m   1459\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mformat(\x1B[38;5;28mformat\x1B[39m)\n' +
      '\x1B[1;32m   1460\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m path \x1B[38;5;129;01mis\x1B[39;00m \x1B[38;5;28;01mNone\x1B[39;00m:\n' +
      '\x1B[0;32m-> 1461\x1B[0m     \x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43m_jwrite\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1462\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m   1463\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39m_jwrite\x1B[38;5;241m.\x1B[39msave(path)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\x1B[0m, in \x1B[0;36mJavaMember.__call__\x1B[0;34m(self, *args)\x1B[0m\n' +
      '\x1B[1;32m   1316\x1B[0m command \x1B[38;5;241m=\x1B[39m proto\x1B[38;5;241m.\x1B[39mCALL_COMMAND_NAME \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1317\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mcommand_header \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1318\x1B[0m     args_command \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1319\x1B[0m     proto\x1B[38;5;241m.\x1B[39mEND_COMMAND_PART\n' +
      '\x1B[1;32m   1321\x1B[0m answer \x1B[38;5;241m=\x1B[39m \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mgateway_client\x1B[38;5;241m.\x1B[39msend_command(command)\n' +
      '\x1B[0;32m-> 1322\x1B[0m return_value \x1B[38;5;241m=\x1B[39m \x1B[43mget_return_value\x1B[49m\x1B[43m(\x1B[49m\n' +
      '\x1B[1;32m   1323\x1B[0m \x1B[43m    \x1B[49m\x1B[43manswer\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mgateway_client\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mtarget_id\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mname\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1325\x1B[0m \x1B[38;5;28;01mfor\x1B[39;00m temp_arg \x1B[38;5;129;01min\x1B[39;00m temp_args:\n' +
      '\x1B[1;32m   1326\x1B[0m     \x1B[38;5;28;01mif\x1B[39;00m \x1B[38;5;28mhasattr\x1B[39m(temp_arg, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m_detach\x1B[39m\x1B[38;5;124m"\x1B[39m):\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\x1B[0m, in \x1B[0;36mcapture_sql_exception.<locals>.deco\x1B[0;34m(*a, **kw)\x1B[0m\n' +
      '\x1B[1;32m    177\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21mdeco\x1B[39m(\x1B[38;5;241m*\x1B[39ma: Any, \x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mkw: Any) \x1B[38;5;241m-\x1B[39m\x1B[38;5;241m>\x1B[39m Any:\n' +
      '\x1B[1;32m    178\x1B[0m     \x1B[38;5;28;01mtry\x1B[39;00m:\n' +
      '\x1B[0;32m--> 179\x1B[0m         \x1B[38;5;28;01mreturn\x1B[39;00m \x1B[43mf\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43ma\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43mkw\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m    180\x1B[0m     \x1B[38;5;28;01mexcept\x1B[39;00m Py4JJavaError \x1B[38;5;28;01mas\x1B[39;00m e:\n' +
      '\x1B[1;32m    181\x1B[0m         converted \x1B[38;5;241m=\x1B[39m convert_exception(e\x1B[38;5;241m.\x1B[39mjava_exception)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\x1B[0m, in \x1B[0;36mget_return_value\x1B[0;34m(answer, gateway_client, target_id, name)\x1B[0m\n' +
      '\x1B[1;32m    324\x1B[0m value \x1B[38;5;241m=\x1B[39m OUTPUT_CONVERTER[\x1B[38;5;28mtype\x1B[39m](answer[\x1B[38;5;241m2\x1B[39m:], gateway_client)\n' +
      '\x1B[1;32m    325\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m answer[\x1B[38;5;241m1\x1B[39m] \x1B[38;5;241m==\x1B[39m REFERENCE_TYPE:\n' +
      '\x1B[0;32m--> 326\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JJavaError(\n' +
      '\x1B[1;32m    327\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m.\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    328\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name), value)\n' +
      '\x1B[1;32m    329\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m    330\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JError(\n' +
      '\x1B[1;32m    331\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m. Trace:\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;132;01m{3}\x1B[39;00m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    332\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name, value))\n',
    '\x1B[0;31mPy4JJavaError\x1B[0m: An error occurred while calling o119.save.\n' +
      ': java.lang.ClassNotFoundException:  com.microsoft.sqlserver\n' +
      '\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n' +
      '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n' +
      '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n' +
      '\tat scala.Option.foreach(Option.scala:407)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
      '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
      '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
      '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
      '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
      '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
      '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
      '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
      '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
      '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
      '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
      '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
      '\tat java.base/java.lang.Thread.run(Thread.java:840)\n'
  ]
}
02:38:51.644 [warn] Cell completed with errors iu [Error]: An error occurred while calling o146.save.
: java.lang.ClassNotFoundException: com.microsoft.sqlserver
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)

    at n.execute (/~/.vscode-server/extensions/ms-toolsai.jupyter-2024.10.0-linux-x64/dist/extension.node.js:297:4958) {
  ename: 'Py4JJavaError',
  evalue: 'An error occurred while calling o146.save.\n' +
    ': java.lang.ClassNotFoundException: com.microsoft.sqlserver\n' +
    '\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n' +
    '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n' +
    '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n' +
    '\tat scala.Option.foreach(Option.scala:407)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
    '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
    '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
    '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
    '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
    '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
    '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
    '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
    '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
    '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
    '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
    '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
    '\tat java.base/java.lang.Thread.run(Thread.java:840)\n',
  traceback: [
    '\x1B[0;31m---------------------------------------------------------------------------\x1B[0m',
    '\x1B[0;31mPy4JJavaError\x1B[0m                             Traceback (most recent call last)',
    'Cell \x1B[0;32mIn[12], line 34\x1B[0m\n' +
      '\x1B[1;32m     32\x1B[0m customer_product_account_data, account_transaction_history_data \x1B[38;5;241m=\x1B[39m generator\x1B[38;5;241m.\x1B[39mgenerate_data(\x1B[38;5;241m20\x1B[39m)\n' +
      '\x1B[1;32m     33\x1B[0m \x1B[38;5;66;03m# Generate and insert data for CustomerProductAccount table\x1B[39;00m\n' +
      "\x1B[0;32m---> 34\x1B[0m \x1B[43minsert_data\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[38;5;124;43mCustomerProductAccount\x1B[39;49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mcustomer_product_account_data\x1B[49m\x1B[43m)\x1B[49m\n" +
      '\x1B[1;32m     35\x1B[0m \x1B[38;5;66;03m# Generate and insert data for AccountTransactionHistory table\x1B[39;00m\n' +
      "\x1B[1;32m     36\x1B[0m insert_data(\x1B[38;5;124m'\x1B[39m\x1B[38;5;124mAccountTransactionHistory\x1B[39m\x1B[38;5;124m'\x1B[39m, account_transaction_history_data)\n",
    'Cell \x1B[0;32mIn[12], line 27\x1B[0m, in \x1B[0;36minsert_data\x1B[0;34m(table_name, data)\x1B[0m\n' +
      '\x1B[1;32m     17\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21minsert_data\x1B[39m(table_name, data):\n' +
      '\x1B[1;32m     18\x1B[0m     df \x1B[38;5;241m=\x1B[39m spark\x1B[38;5;241m.\x1B[39mcreateDataFrame([Row(\x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mi) \x1B[38;5;28;01mfor\x1B[39;00m i \x1B[38;5;129;01min\x1B[39;00m data])\n' +
      '\x1B[1;32m     19\x1B[0m     \x1B[43mdf\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mwrite\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     20\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mformat\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mjdbc\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     21\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdriver\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mcom.microsoft.sqlserver\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     22\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43murl\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mjdbc_url\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     23\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdbtable\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mtable_name\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     24\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43muser\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43musername\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     25\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mpassword\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mpassword\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     26\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mmode\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mappend\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[0;32m---> 27\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1461\x1B[0m, in \x1B[0;36mDataFrameWriter.save\x1B[0;34m(self, path, format, mode, partitionBy, **options)\x1B[0m\n' +
      '\x1B[1;32m   1459\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mformat(\x1B[38;5;28mformat\x1B[39m)\n' +
      '\x1B[1;32m   1460\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m path \x1B[38;5;129;01mis\x1B[39;00m \x1B[38;5;28;01mNone\x1B[39;00m:\n' +
      '\x1B[0;32m-> 1461\x1B[0m     \x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43m_jwrite\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1462\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m   1463\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39m_jwrite\x1B[38;5;241m.\x1B[39msave(path)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\x1B[0m, in \x1B[0;36mJavaMember.__call__\x1B[0;34m(self, *args)\x1B[0m\n' +
      '\x1B[1;32m   1316\x1B[0m command \x1B[38;5;241m=\x1B[39m proto\x1B[38;5;241m.\x1B[39mCALL_COMMAND_NAME \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1317\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mcommand_header \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1318\x1B[0m     args_command \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1319\x1B[0m     proto\x1B[38;5;241m.\x1B[39mEND_COMMAND_PART\n' +
      '\x1B[1;32m   1321\x1B[0m answer \x1B[38;5;241m=\x1B[39m \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mgateway_client\x1B[38;5;241m.\x1B[39msend_command(command)\n' +
      '\x1B[0;32m-> 1322\x1B[0m return_value \x1B[38;5;241m=\x1B[39m \x1B[43mget_return_value\x1B[49m\x1B[43m(\x1B[49m\n' +
      '\x1B[1;32m   1323\x1B[0m \x1B[43m    \x1B[49m\x1B[43manswer\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mgateway_client\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mtarget_id\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mname\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1325\x1B[0m \x1B[38;5;28;01mfor\x1B[39;00m temp_arg \x1B[38;5;129;01min\x1B[39;00m temp_args:\n' +
      '\x1B[1;32m   1326\x1B[0m     \x1B[38;5;28;01mif\x1B[39;00m \x1B[38;5;28mhasattr\x1B[39m(temp_arg, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m_detach\x1B[39m\x1B[38;5;124m"\x1B[39m):\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\x1B[0m, in \x1B[0;36mcapture_sql_exception.<locals>.deco\x1B[0;34m(*a, **kw)\x1B[0m\n' +
      '\x1B[1;32m    177\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21mdeco\x1B[39m(\x1B[38;5;241m*\x1B[39ma: Any, \x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mkw: Any) \x1B[38;5;241m-\x1B[39m\x1B[38;5;241m>\x1B[39m Any:\n' +
      '\x1B[1;32m    178\x1B[0m     \x1B[38;5;28;01mtry\x1B[39;00m:\n' +
      '\x1B[0;32m--> 179\x1B[0m         \x1B[38;5;28;01mreturn\x1B[39;00m \x1B[43mf\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43ma\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43mkw\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m    180\x1B[0m     \x1B[38;5;28;01mexcept\x1B[39;00m Py4JJavaError \x1B[38;5;28;01mas\x1B[39;00m e:\n' +
      '\x1B[1;32m    181\x1B[0m         converted \x1B[38;5;241m=\x1B[39m convert_exception(e\x1B[38;5;241m.\x1B[39mjava_exception)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\x1B[0m, in \x1B[0;36mget_return_value\x1B[0;34m(answer, gateway_client, target_id, name)\x1B[0m\n' +
      '\x1B[1;32m    324\x1B[0m value \x1B[38;5;241m=\x1B[39m OUTPUT_CONVERTER[\x1B[38;5;28mtype\x1B[39m](answer[\x1B[38;5;241m2\x1B[39m:], gateway_client)\n' +
      '\x1B[1;32m    325\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m answer[\x1B[38;5;241m1\x1B[39m] \x1B[38;5;241m==\x1B[39m REFERENCE_TYPE:\n' +
      '\x1B[0;32m--> 326\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JJavaError(\n' +
      '\x1B[1;32m    327\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m.\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    328\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name), value)\n' +
      '\x1B[1;32m    329\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m    330\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JError(\n' +
      '\x1B[1;32m    331\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m. Trace:\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;132;01m{3}\x1B[39;00m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    332\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name, value))\n',
    '\x1B[0;31mPy4JJavaError\x1B[0m: An error occurred while calling o146.save.\n' +
      ': java.lang.ClassNotFoundException: com.microsoft.sqlserver\n' +
      '\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n' +
      '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n' +
      '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n' +
      '\tat scala.Option.foreach(Option.scala:407)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
      '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
      '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
      '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
      '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
      '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
      '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
      '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
      '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
      '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
      '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
      '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
      '\tat java.base/java.lang.Thread.run(Thread.java:840)\n'
  ]
}
02:46:20.305 [info] Restart requested /container/pyspark_workspace/source_code/sample_code/Test_Project_1.ipynb
02:46:20.458 [warn] Failed to get activated env vars for /usr/local/bin/python in 117ms
02:46:20.481 [info] Process Execution: /usr/local/bin/python -c "import ipykernel; print(ipykernel.__version__); print("5dc3a68c-e34e-4080-9c3e-2a532b2ccb4d"); print(ipykernel.__file__)"
02:46:20.513 [info] Process Execution: /usr/local/bin/python -m ipykernel_launcher --f=/~/.local/share/jupyter/runtime/kernel-v32fa0ee248f35c04a218f71fcb82ad904e8492158.json
    > cwd: //container/pyspark_workspace/source_code/sample_code
02:46:23.142 [info] Restarted 6c8c97de-d785-4080-b9e4-55baf67ded63
02:47:03.380 [warn] Cell completed with errors iu [Error]: An error occurred while calling o68.save.
: java.lang.ClassNotFoundException: com.microsoft.sqlserver
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)

    at n.execute (/~/.vscode-server/extensions/ms-toolsai.jupyter-2024.10.0-linux-x64/dist/extension.node.js:297:4958) {
  ename: 'Py4JJavaError',
  evalue: 'An error occurred while calling o68.save.\n' +
    ': java.lang.ClassNotFoundException: com.microsoft.sqlserver\n' +
    '\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n' +
    '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n' +
    '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n' +
    '\tat scala.Option.foreach(Option.scala:407)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
    '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
    '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
    '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
    '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
    '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
    '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
    '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
    '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
    '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
    '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
    '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
    '\tat java.base/java.lang.Thread.run(Thread.java:840)\n',
  traceback: [
    '\x1B[0;31m---------------------------------------------------------------------------\x1B[0m',
    '\x1B[0;31mPy4JJavaError\x1B[0m                             Traceback (most recent call last)',
    'Cell \x1B[0;32mIn[9], line 34\x1B[0m\n' +
      '\x1B[1;32m     32\x1B[0m customer_product_account_data, account_transaction_history_data \x1B[38;5;241m=\x1B[39m generator\x1B[38;5;241m.\x1B[39mgenerate_data(\x1B[38;5;241m20\x1B[39m)\n' +
      '\x1B[1;32m     33\x1B[0m \x1B[38;5;66;03m# Generate and insert data for CustomerProductAccount table\x1B[39;00m\n' +
      "\x1B[0;32m---> 34\x1B[0m \x1B[43minsert_data\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[38;5;124;43mCustomerProductAccount\x1B[39;49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mcustomer_product_account_data\x1B[49m\x1B[43m)\x1B[49m\n" +
      '\x1B[1;32m     35\x1B[0m \x1B[38;5;66;03m# Generate and insert data for AccountTransactionHistory table\x1B[39;00m\n' +
      "\x1B[1;32m     36\x1B[0m insert_data(\x1B[38;5;124m'\x1B[39m\x1B[38;5;124mAccountTransactionHistory\x1B[39m\x1B[38;5;124m'\x1B[39m, account_transaction_history_data)\n",
    'Cell \x1B[0;32mIn[9], line 27\x1B[0m, in \x1B[0;36minsert_data\x1B[0;34m(table_name, data)\x1B[0m\n' +
      '\x1B[1;32m     17\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21minsert_data\x1B[39m(table_name, data):\n' +
      '\x1B[1;32m     18\x1B[0m     df \x1B[38;5;241m=\x1B[39m spark\x1B[38;5;241m.\x1B[39mcreateDataFrame([Row(\x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mi) \x1B[38;5;28;01mfor\x1B[39;00m i \x1B[38;5;129;01min\x1B[39;00m data])\n' +
      '\x1B[1;32m     19\x1B[0m     \x1B[43mdf\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mwrite\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     20\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mformat\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mjdbc\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     21\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdriver\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mcom.microsoft.sqlserver\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     22\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43murl\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mjdbc_url\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     23\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdbtable\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mtable_name\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     24\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43muser\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43musername\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     25\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mpassword\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mpassword\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     26\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mmode\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mappend\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[0;32m---> 27\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1461\x1B[0m, in \x1B[0;36mDataFrameWriter.save\x1B[0;34m(self, path, format, mode, partitionBy, **options)\x1B[0m\n' +
      '\x1B[1;32m   1459\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mformat(\x1B[38;5;28mformat\x1B[39m)\n' +
      '\x1B[1;32m   1460\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m path \x1B[38;5;129;01mis\x1B[39;00m \x1B[38;5;28;01mNone\x1B[39;00m:\n' +
      '\x1B[0;32m-> 1461\x1B[0m     \x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43m_jwrite\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1462\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m   1463\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39m_jwrite\x1B[38;5;241m.\x1B[39msave(path)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\x1B[0m, in \x1B[0;36mJavaMember.__call__\x1B[0;34m(self, *args)\x1B[0m\n' +
      '\x1B[1;32m   1316\x1B[0m command \x1B[38;5;241m=\x1B[39m proto\x1B[38;5;241m.\x1B[39mCALL_COMMAND_NAME \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1317\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mcommand_header \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1318\x1B[0m     args_command \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1319\x1B[0m     proto\x1B[38;5;241m.\x1B[39mEND_COMMAND_PART\n' +
      '\x1B[1;32m   1321\x1B[0m answer \x1B[38;5;241m=\x1B[39m \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mgateway_client\x1B[38;5;241m.\x1B[39msend_command(command)\n' +
      '\x1B[0;32m-> 1322\x1B[0m return_value \x1B[38;5;241m=\x1B[39m \x1B[43mget_return_value\x1B[49m\x1B[43m(\x1B[49m\n' +
      '\x1B[1;32m   1323\x1B[0m \x1B[43m    \x1B[49m\x1B[43manswer\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mgateway_client\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mtarget_id\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mname\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1325\x1B[0m \x1B[38;5;28;01mfor\x1B[39;00m temp_arg \x1B[38;5;129;01min\x1B[39;00m temp_args:\n' +
      '\x1B[1;32m   1326\x1B[0m     \x1B[38;5;28;01mif\x1B[39;00m \x1B[38;5;28mhasattr\x1B[39m(temp_arg, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m_detach\x1B[39m\x1B[38;5;124m"\x1B[39m):\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\x1B[0m, in \x1B[0;36mcapture_sql_exception.<locals>.deco\x1B[0;34m(*a, **kw)\x1B[0m\n' +
      '\x1B[1;32m    177\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21mdeco\x1B[39m(\x1B[38;5;241m*\x1B[39ma: Any, \x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mkw: Any) \x1B[38;5;241m-\x1B[39m\x1B[38;5;241m>\x1B[39m Any:\n' +
      '\x1B[1;32m    178\x1B[0m     \x1B[38;5;28;01mtry\x1B[39;00m:\n' +
      '\x1B[0;32m--> 179\x1B[0m         \x1B[38;5;28;01mreturn\x1B[39;00m \x1B[43mf\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43ma\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43mkw\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m    180\x1B[0m     \x1B[38;5;28;01mexcept\x1B[39;00m Py4JJavaError \x1B[38;5;28;01mas\x1B[39;00m e:\n' +
      '\x1B[1;32m    181\x1B[0m         converted \x1B[38;5;241m=\x1B[39m convert_exception(e\x1B[38;5;241m.\x1B[39mjava_exception)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\x1B[0m, in \x1B[0;36mget_return_value\x1B[0;34m(answer, gateway_client, target_id, name)\x1B[0m\n' +
      '\x1B[1;32m    324\x1B[0m value \x1B[38;5;241m=\x1B[39m OUTPUT_CONVERTER[\x1B[38;5;28mtype\x1B[39m](answer[\x1B[38;5;241m2\x1B[39m:], gateway_client)\n' +
      '\x1B[1;32m    325\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m answer[\x1B[38;5;241m1\x1B[39m] \x1B[38;5;241m==\x1B[39m REFERENCE_TYPE:\n' +
      '\x1B[0;32m--> 326\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JJavaError(\n' +
      '\x1B[1;32m    327\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m.\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    328\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name), value)\n' +
      '\x1B[1;32m    329\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m    330\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JError(\n' +
      '\x1B[1;32m    331\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m. Trace:\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;132;01m{3}\x1B[39;00m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    332\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name, value))\n',
    '\x1B[0;31mPy4JJavaError\x1B[0m: An error occurred while calling o68.save.\n' +
      ': java.lang.ClassNotFoundException: com.microsoft.sqlserver\n' +
      '\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n' +
      '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n' +
      '\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n' +
      '\tat scala.Option.foreach(Option.scala:407)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
      '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
      '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
      '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
      '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
      '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
      '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
      '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
      '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
      '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
      '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
      '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
      '\tat java.base/java.lang.Thread.run(Thread.java:840)\n'
  ]
}
02:47:20.887 [warn] Cell completed with errors iu [Error]: An error occurred while calling o94.save.
: java.sql.SQLException: No suitable driver
	at java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)

    at n.execute (/~/.vscode-server/extensions/ms-toolsai.jupyter-2024.10.0-linux-x64/dist/extension.node.js:297:4958) {
  ename: 'Py4JJavaError',
  evalue: 'An error occurred while calling o94.save.\n' +
    ': java.sql.SQLException: No suitable driver\n' +
    '\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\n' +
    '\tat scala.Option.getOrElse(Option.scala:189)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
    '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
    '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
    '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
    '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
    '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
    '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
    '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
    '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
    '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
    '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
    '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
    '\tat java.base/java.lang.Thread.run(Thread.java:840)\n',
  traceback: [
    '\x1B[0;31m---------------------------------------------------------------------------\x1B[0m',
    '\x1B[0;31mPy4JJavaError\x1B[0m                             Traceback (most recent call last)',
    'Cell \x1B[0;32mIn[10], line 33\x1B[0m\n' +
      '\x1B[1;32m     31\x1B[0m customer_product_account_data, account_transaction_history_data \x1B[38;5;241m=\x1B[39m generator\x1B[38;5;241m.\x1B[39mgenerate_data(\x1B[38;5;241m20\x1B[39m)\n' +
      '\x1B[1;32m     32\x1B[0m \x1B[38;5;66;03m# Generate and insert data for CustomerProductAccount table\x1B[39;00m\n' +
      "\x1B[0;32m---> 33\x1B[0m \x1B[43minsert_data\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[38;5;124;43mCustomerProductAccount\x1B[39;49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mcustomer_product_account_data\x1B[49m\x1B[43m)\x1B[49m\n" +
      '\x1B[1;32m     34\x1B[0m \x1B[38;5;66;03m# Generate and insert data for AccountTransactionHistory table\x1B[39;00m\n' +
      "\x1B[1;32m     35\x1B[0m insert_data(\x1B[38;5;124m'\x1B[39m\x1B[38;5;124mAccountTransactionHistory\x1B[39m\x1B[38;5;124m'\x1B[39m, account_transaction_history_data)\n",
    'Cell \x1B[0;32mIn[10], line 26\x1B[0m, in \x1B[0;36minsert_data\x1B[0;34m(table_name, data)\x1B[0m\n' +
      '\x1B[1;32m     17\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21minsert_data\x1B[39m(table_name, data):\n' +
      '\x1B[1;32m     18\x1B[0m     df \x1B[38;5;241m=\x1B[39m spark\x1B[38;5;241m.\x1B[39mcreateDataFrame([Row(\x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mi) \x1B[38;5;28;01mfor\x1B[39;00m i \x1B[38;5;129;01min\x1B[39;00m data])\n' +
      '\x1B[1;32m     19\x1B[0m     \x1B[43mdf\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mwrite\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     20\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mformat\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mjdbc\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     21\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43murl\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mjdbc_url\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     22\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdbtable\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mtable_name\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     23\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43muser\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43musername\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     24\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mpassword\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mpassword\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     25\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mmode\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mappend\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[0;32m---> 26\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1461\x1B[0m, in \x1B[0;36mDataFrameWriter.save\x1B[0;34m(self, path, format, mode, partitionBy, **options)\x1B[0m\n' +
      '\x1B[1;32m   1459\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mformat(\x1B[38;5;28mformat\x1B[39m)\n' +
      '\x1B[1;32m   1460\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m path \x1B[38;5;129;01mis\x1B[39;00m \x1B[38;5;28;01mNone\x1B[39;00m:\n' +
      '\x1B[0;32m-> 1461\x1B[0m     \x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43m_jwrite\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1462\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m   1463\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39m_jwrite\x1B[38;5;241m.\x1B[39msave(path)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\x1B[0m, in \x1B[0;36mJavaMember.__call__\x1B[0;34m(self, *args)\x1B[0m\n' +
      '\x1B[1;32m   1316\x1B[0m command \x1B[38;5;241m=\x1B[39m proto\x1B[38;5;241m.\x1B[39mCALL_COMMAND_NAME \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1317\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mcommand_header \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1318\x1B[0m     args_command \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1319\x1B[0m     proto\x1B[38;5;241m.\x1B[39mEND_COMMAND_PART\n' +
      '\x1B[1;32m   1321\x1B[0m answer \x1B[38;5;241m=\x1B[39m \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mgateway_client\x1B[38;5;241m.\x1B[39msend_command(command)\n' +
      '\x1B[0;32m-> 1322\x1B[0m return_value \x1B[38;5;241m=\x1B[39m \x1B[43mget_return_value\x1B[49m\x1B[43m(\x1B[49m\n' +
      '\x1B[1;32m   1323\x1B[0m \x1B[43m    \x1B[49m\x1B[43manswer\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mgateway_client\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mtarget_id\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mname\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1325\x1B[0m \x1B[38;5;28;01mfor\x1B[39;00m temp_arg \x1B[38;5;129;01min\x1B[39;00m temp_args:\n' +
      '\x1B[1;32m   1326\x1B[0m     \x1B[38;5;28;01mif\x1B[39;00m \x1B[38;5;28mhasattr\x1B[39m(temp_arg, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m_detach\x1B[39m\x1B[38;5;124m"\x1B[39m):\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\x1B[0m, in \x1B[0;36mcapture_sql_exception.<locals>.deco\x1B[0;34m(*a, **kw)\x1B[0m\n' +
      '\x1B[1;32m    177\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21mdeco\x1B[39m(\x1B[38;5;241m*\x1B[39ma: Any, \x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mkw: Any) \x1B[38;5;241m-\x1B[39m\x1B[38;5;241m>\x1B[39m Any:\n' +
      '\x1B[1;32m    178\x1B[0m     \x1B[38;5;28;01mtry\x1B[39;00m:\n' +
      '\x1B[0;32m--> 179\x1B[0m         \x1B[38;5;28;01mreturn\x1B[39;00m \x1B[43mf\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43ma\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43mkw\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m    180\x1B[0m     \x1B[38;5;28;01mexcept\x1B[39;00m Py4JJavaError \x1B[38;5;28;01mas\x1B[39;00m e:\n' +
      '\x1B[1;32m    181\x1B[0m         converted \x1B[38;5;241m=\x1B[39m convert_exception(e\x1B[38;5;241m.\x1B[39mjava_exception)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\x1B[0m, in \x1B[0;36mget_return_value\x1B[0;34m(answer, gateway_client, target_id, name)\x1B[0m\n' +
      '\x1B[1;32m    324\x1B[0m value \x1B[38;5;241m=\x1B[39m OUTPUT_CONVERTER[\x1B[38;5;28mtype\x1B[39m](answer[\x1B[38;5;241m2\x1B[39m:], gateway_client)\n' +
      '\x1B[1;32m    325\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m answer[\x1B[38;5;241m1\x1B[39m] \x1B[38;5;241m==\x1B[39m REFERENCE_TYPE:\n' +
      '\x1B[0;32m--> 326\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JJavaError(\n' +
      '\x1B[1;32m    327\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m.\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    328\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name), value)\n' +
      '\x1B[1;32m    329\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m    330\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JError(\n' +
      '\x1B[1;32m    331\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m. Trace:\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;132;01m{3}\x1B[39;00m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    332\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name, value))\n',
    '\x1B[0;31mPy4JJavaError\x1B[0m: An error occurred while calling o94.save.\n' +
      ': java.sql.SQLException: No suitable driver\n' +
      '\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:299)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\n' +
      '\tat scala.Option.getOrElse(Option.scala:189)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:254)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:258)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n' +
      '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
      '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
      '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
      '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
      '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
      '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
      '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
      '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
      '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
      '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
      '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
      '\tat java.base/java.lang.Thread.run(Thread.java:840)\n'
  ]
}
03:01:52.046 [info] Restart requested /container/pyspark_workspace/source_code/sample_code/Test_Project_1.ipynb
03:01:52.268 [warn] Failed to get activated env vars for /usr/local/bin/python in 199ms
03:01:52.309 [info] Process Execution: /usr/local/bin/python -c "import ipykernel; print(ipykernel.__version__); print("5dc3a68c-e34e-4080-9c3e-2a532b2ccb4d"); print(ipykernel.__file__)"
03:01:52.375 [info] Process Execution: /usr/local/bin/python -m ipykernel_launcher --f=/~/.local/share/jupyter/runtime/kernel-v30b2b582e2d6ea6a1bdac39bce8fbb10c8b74d9f5.json
    > cwd: //container/pyspark_workspace/source_code/sample_code
03:01:54.982 [info] Restarted 6c8c97de-d785-4080-b9e4-55baf67ded63
03:02:23.993 [warn] Cell completed with errors iu [Error]: An error occurred while calling o67.save.
: com.microsoft.sqlserver.jdbc.SQLServerException: "encrypt" property is set to "true" and "trustServerCertificate" property is set to "false" but the driver could not establish a secure connection to SQL Server by using Secure Sockets Layer (SSL) encryption: Error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target. ClientConnectionId:1e048493-60c1-4ea0-9026-1e40dff9d7bc
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:4271)
	at com.microsoft.sqlserver.jdbc.TDSChannel.enableSSL(IOBuffer.java:1965)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:3797)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:3385)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:3194)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1971)
	at com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:1263)
	at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
	at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)
	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)
	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: javax.net.ssl.SSLHandshakeException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
	at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:131)
	at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:378)
	at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:321)
	at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:316)
	at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:654)
	at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.onCertificate(CertificateMessage.java:473)
	at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.consume(CertificateMessage.java:369)
	at java.base/sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:396)
	at java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:480)
	at java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:458)
	at java.base/sun.security.ssl.TransportContext.dispatch(TransportContext.java:201)
	at java.base/sun.security.ssl.SSLTransport.decode(SSLTransport.java:172)
	at java.base/sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1510)
	at java.base/sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1425)
	at java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:455)
	at java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:426)
	at com.microsoft.sqlserver.jdbc.TDSChannel.enableSSL(IOBuffer.java:1854)
	... 51 more
Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
	at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:439)
	at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:306)
	at java.base/sun.security.validator.Validator.validate(Validator.java:264)
	at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:242)
	at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:113)
	at com.microsoft.sqlserver.jdbc.HostNameOverrideX509TrustManager.checkServerTrusted(SQLServerTrustManager.java:88)
	at java.base/sun.security.ssl.AbstractTrustManagerWrapper.checkServerTrusted(SSLContextImpl.java:1439)
	at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:638)
	... 63 more
Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
	at java.base/sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:148)
	at java.base/sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:129)
	at java.base/java.security.cert.CertPathBuilder.build(CertPathBuilder.java:297)
	at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:434)
	... 70 more

    at n.execute (/~/.vscode-server/extensions/ms-toolsai.jupyter-2024.10.0-linux-x64/dist/extension.node.js:297:4958) {
  ename: 'Py4JJavaError',
  evalue: 'An error occurred while calling o67.save.\n' +
    ': com.microsoft.sqlserver.jdbc.SQLServerException: "encrypt" property is set to "true" and "trustServerCertificate" property is set to "false" but the driver could not establish a secure connection to SQL Server by using Secure Sockets Layer (SSL) encryption: Error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target. ClientConnectionId:1e048493-60c1-4ea0-9026-1e40dff9d7bc\n' +
    '\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:4271)\n' +
    '\tat com.microsoft.sqlserver.jdbc.TDSChannel.enableSSL(IOBuffer.java:1965)\n' +
    '\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:3797)\n' +
    '\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:3385)\n' +
    '\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:3194)\n' +
    '\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1971)\n' +
    '\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:1263)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n' +
    '\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n' +
    '\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)\n' +
    '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
    '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
    '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
    '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
    '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
    '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
    '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
    '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
    '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
    '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
    '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
    '\tat java.base/java.lang.Thread.run(Thread.java:840)\n' +
    'Caused by: javax.net.ssl.SSLHandshakeException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n' +
    '\tat java.base/sun.security.ssl.Alert.createSSLException(Alert.java:131)\n' +
    '\tat java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:378)\n' +
    '\tat java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:321)\n' +
    '\tat java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:316)\n' +
    '\tat java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:654)\n' +
    '\tat java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.onCertificate(CertificateMessage.java:473)\n' +
    '\tat java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.consume(CertificateMessage.java:369)\n' +
    '\tat java.base/sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:396)\n' +
    '\tat java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:480)\n' +
    '\tat java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:458)\n' +
    '\tat java.base/sun.security.ssl.TransportContext.dispatch(TransportContext.java:201)\n' +
    '\tat java.base/sun.security.ssl.SSLTransport.decode(SSLTransport.java:172)\n' +
    '\tat java.base/sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1510)\n' +
    '\tat java.base/sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1425)\n' +
    '\tat java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:455)\n' +
    '\tat java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:426)\n' +
    '\tat com.microsoft.sqlserver.jdbc.TDSChannel.enableSSL(IOBuffer.java:1854)\n' +
    '\t... 51 more\n' +
    'Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n' +
    '\tat java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:439)\n' +
    '\tat java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:306)\n' +
    '\tat java.base/sun.security.validator.Validator.validate(Validator.java:264)\n' +
    '\tat java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:242)\n' +
    '\tat java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:113)\n' +
    '\tat com.microsoft.sqlserver.jdbc.HostNameOverrideX509TrustManager.checkServerTrusted(SQLServerTrustManager.java:88)\n' +
    '\tat java.base/sun.security.ssl.AbstractTrustManagerWrapper.checkServerTrusted(SSLContextImpl.java:1439)\n' +
    '\tat java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:638)\n' +
    '\t... 63 more\n' +
    'Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n' +
    '\tat java.base/sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:148)\n' +
    '\tat java.base/sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:129)\n' +
    '\tat java.base/java.security.cert.CertPathBuilder.build(CertPathBuilder.java:297)\n' +
    '\tat java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:434)\n' +
    '\t... 70 more\n',
  traceback: [
    '\x1B[0;31m---------------------------------------------------------------------------\x1B[0m',
    '\x1B[0;31mPy4JJavaError\x1B[0m                             Traceback (most recent call last)',
    'Cell \x1B[0;32mIn[6], line 33\x1B[0m\n' +
      '\x1B[1;32m     31\x1B[0m customer_product_account_data, account_transaction_history_data \x1B[38;5;241m=\x1B[39m generator\x1B[38;5;241m.\x1B[39mgenerate_data(\x1B[38;5;241m20\x1B[39m)\n' +
      '\x1B[1;32m     32\x1B[0m \x1B[38;5;66;03m# Generate and insert data for CustomerProductAccount table\x1B[39;00m\n' +
      "\x1B[0;32m---> 33\x1B[0m \x1B[43minsert_data\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[38;5;124;43mCustomerProductAccount\x1B[39;49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mcustomer_product_account_data\x1B[49m\x1B[43m)\x1B[49m\n" +
      '\x1B[1;32m     34\x1B[0m \x1B[38;5;66;03m# Generate and insert data for AccountTransactionHistory table\x1B[39;00m\n' +
      "\x1B[1;32m     35\x1B[0m insert_data(\x1B[38;5;124m'\x1B[39m\x1B[38;5;124mAccountTransactionHistory\x1B[39m\x1B[38;5;124m'\x1B[39m, account_transaction_history_data)\n",
    'Cell \x1B[0;32mIn[6], line 26\x1B[0m, in \x1B[0;36minsert_data\x1B[0;34m(table_name, data)\x1B[0m\n' +
      '\x1B[1;32m     17\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21minsert_data\x1B[39m(table_name, data):\n' +
      '\x1B[1;32m     18\x1B[0m     df \x1B[38;5;241m=\x1B[39m spark\x1B[38;5;241m.\x1B[39mcreateDataFrame([Row(\x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mi) \x1B[38;5;28;01mfor\x1B[39;00m i \x1B[38;5;129;01min\x1B[39;00m data])\n' +
      '\x1B[1;32m     19\x1B[0m     \x1B[43mdf\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mwrite\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     20\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mformat\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mjdbc\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     21\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43murl\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mjdbc_url\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     22\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdbtable\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mtable_name\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     23\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43muser\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43musername\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     24\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mpassword\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mpassword\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     25\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mmode\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mappend\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[0;32m---> 26\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1461\x1B[0m, in \x1B[0;36mDataFrameWriter.save\x1B[0;34m(self, path, format, mode, partitionBy, **options)\x1B[0m\n' +
      '\x1B[1;32m   1459\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mformat(\x1B[38;5;28mformat\x1B[39m)\n' +
      '\x1B[1;32m   1460\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m path \x1B[38;5;129;01mis\x1B[39;00m \x1B[38;5;28;01mNone\x1B[39;00m:\n' +
      '\x1B[0;32m-> 1461\x1B[0m     \x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43m_jwrite\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1462\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m   1463\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39m_jwrite\x1B[38;5;241m.\x1B[39msave(path)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\x1B[0m, in \x1B[0;36mJavaMember.__call__\x1B[0;34m(self, *args)\x1B[0m\n' +
      '\x1B[1;32m   1316\x1B[0m command \x1B[38;5;241m=\x1B[39m proto\x1B[38;5;241m.\x1B[39mCALL_COMMAND_NAME \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1317\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mcommand_header \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1318\x1B[0m     args_command \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1319\x1B[0m     proto\x1B[38;5;241m.\x1B[39mEND_COMMAND_PART\n' +
      '\x1B[1;32m   1321\x1B[0m answer \x1B[38;5;241m=\x1B[39m \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mgateway_client\x1B[38;5;241m.\x1B[39msend_command(command)\n' +
      '\x1B[0;32m-> 1322\x1B[0m return_value \x1B[38;5;241m=\x1B[39m \x1B[43mget_return_value\x1B[49m\x1B[43m(\x1B[49m\n' +
      '\x1B[1;32m   1323\x1B[0m \x1B[43m    \x1B[49m\x1B[43manswer\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mgateway_client\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mtarget_id\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mname\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1325\x1B[0m \x1B[38;5;28;01mfor\x1B[39;00m temp_arg \x1B[38;5;129;01min\x1B[39;00m temp_args:\n' +
      '\x1B[1;32m   1326\x1B[0m     \x1B[38;5;28;01mif\x1B[39;00m \x1B[38;5;28mhasattr\x1B[39m(temp_arg, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m_detach\x1B[39m\x1B[38;5;124m"\x1B[39m):\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\x1B[0m, in \x1B[0;36mcapture_sql_exception.<locals>.deco\x1B[0;34m(*a, **kw)\x1B[0m\n' +
      '\x1B[1;32m    177\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21mdeco\x1B[39m(\x1B[38;5;241m*\x1B[39ma: Any, \x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mkw: Any) \x1B[38;5;241m-\x1B[39m\x1B[38;5;241m>\x1B[39m Any:\n' +
      '\x1B[1;32m    178\x1B[0m     \x1B[38;5;28;01mtry\x1B[39;00m:\n' +
      '\x1B[0;32m--> 179\x1B[0m         \x1B[38;5;28;01mreturn\x1B[39;00m \x1B[43mf\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43ma\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43mkw\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m    180\x1B[0m     \x1B[38;5;28;01mexcept\x1B[39;00m Py4JJavaError \x1B[38;5;28;01mas\x1B[39;00m e:\n' +
      '\x1B[1;32m    181\x1B[0m         converted \x1B[38;5;241m=\x1B[39m convert_exception(e\x1B[38;5;241m.\x1B[39mjava_exception)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\x1B[0m, in \x1B[0;36mget_return_value\x1B[0;34m(answer, gateway_client, target_id, name)\x1B[0m\n' +
      '\x1B[1;32m    324\x1B[0m value \x1B[38;5;241m=\x1B[39m OUTPUT_CONVERTER[\x1B[38;5;28mtype\x1B[39m](answer[\x1B[38;5;241m2\x1B[39m:], gateway_client)\n' +
      '\x1B[1;32m    325\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m answer[\x1B[38;5;241m1\x1B[39m] \x1B[38;5;241m==\x1B[39m REFERENCE_TYPE:\n' +
      '\x1B[0;32m--> 326\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JJavaError(\n' +
      '\x1B[1;32m    327\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m.\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    328\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name), value)\n' +
      '\x1B[1;32m    329\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m    330\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JError(\n' +
      '\x1B[1;32m    331\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m. Trace:\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;132;01m{3}\x1B[39;00m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    332\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name, value))\n',
    '\x1B[0;31mPy4JJavaError\x1B[0m: An error occurred while calling o67.save.\n' +
      ': com.microsoft.sqlserver.jdbc.SQLServerException: "encrypt" property is set to "true" and "trustServerCertificate" property is set to "false" but the driver could not establish a secure connection to SQL Server by using Secure Sockets Layer (SSL) encryption: Error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target. ClientConnectionId:1e048493-60c1-4ea0-9026-1e40dff9d7bc\n' +
      '\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:4271)\n' +
      '\tat com.microsoft.sqlserver.jdbc.TDSChannel.enableSSL(IOBuffer.java:1965)\n' +
      '\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:3797)\n' +
      '\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:3385)\n' +
      '\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:3194)\n' +
      '\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1971)\n' +
      '\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:1263)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n' +
      '\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:160)\n' +
      '\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:156)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)\n' +
      '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
      '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
      '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
      '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
      '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
      '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
      '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
      '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
      '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
      '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
      '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
      '\tat java.base/java.lang.Thread.run(Thread.java:840)\n' +
      'Caused by: javax.net.ssl.SSLHandshakeException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n' +
      '\tat java.base/sun.security.ssl.Alert.createSSLException(Alert.java:131)\n' +
      '\tat java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:378)\n' +
      '\tat java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:321)\n' +
      '\tat java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:316)\n' +
      '\tat java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:654)\n' +
      '\tat java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.onCertificate(CertificateMessage.java:473)\n' +
      '\tat java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.consume(CertificateMessage.java:369)\n' +
      '\tat java.base/sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:396)\n' +
      '\tat java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:480)\n' +
      '\tat java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:458)\n' +
      '\tat java.base/sun.security.ssl.TransportContext.dispatch(TransportContext.java:201)\n' +
      '\tat java.base/sun.security.ssl.SSLTransport.decode(SSLTransport.java:172)\n' +
      '\tat java.base/sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1510)\n' +
      '\tat java.base/sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1425)\n' +
      '\tat java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:455)\n' +
      '\tat java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:426)\n' +
      '\tat com.microsoft.sqlserver.jdbc.TDSChannel.enableSSL(IOBuffer.java:1854)\n' +
      '\t... 51 more\n' +
      'Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n' +
      '\tat java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:439)\n' +
      '\tat java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:306)\n' +
      '\tat java.base/sun.security.validator.Validator.validate(Validator.java:264)\n' +
      '\tat java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:242)\n' +
      '\tat java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:113)\n' +
      '\tat com.microsoft.sqlserver.jdbc.HostNameOverrideX509TrustManager.checkServerTrusted(SQLServerTrustManager.java:88)\n' +
      '\tat java.base/sun.security.ssl.AbstractTrustManagerWrapper.checkServerTrusted(SSLContextImpl.java:1439)\n' +
      '\tat java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:638)\n' +
      '\t... 63 more\n' +
      'Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n' +
      '\tat java.base/sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:148)\n' +
      '\tat java.base/sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:129)\n' +
      '\tat java.base/java.security.cert.CertPathBuilder.build(CertPathBuilder.java:297)\n' +
      '\tat java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:434)\n' +
      '\t... 70 more\n'
  ]
}
03:07:07.282 [warn] Cell completed with errors iu [Error]: name 'true' is not defined
    at n.execute (/~/.vscode-server/extensions/ms-toolsai.jupyter-2024.10.0-linux-x64/dist/extension.node.js:297:4958) {
  ename: 'NameError',
  evalue: "name 'true' is not defined",
  traceback: [
    '\x1B[0;31m---------------------------------------------------------------------------\x1B[0m',
    '\x1B[0;31mNameError\x1B[0m                                 Traceback (most recent call last)',
    'Cell \x1B[0;32mIn[7], line 35\x1B[0m\n' +
      '\x1B[1;32m     33\x1B[0m customer_product_account_data, account_transaction_history_data \x1B[38;5;241m=\x1B[39m generator\x1B[38;5;241m.\x1B[39mgenerate_data(\x1B[38;5;241m20\x1B[39m)\n' +
      '\x1B[1;32m     34\x1B[0m \x1B[38;5;66;03m# Generate and insert data for CustomerProductAccount table\x1B[39;00m\n' +
      "\x1B[0;32m---> 35\x1B[0m \x1B[43minsert_data\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[38;5;124;43mCustomerProductAccount\x1B[39;49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mcustomer_product_account_data\x1B[49m\x1B[43m)\x1B[49m\n" +
      '\x1B[1;32m     36\x1B[0m \x1B[38;5;66;03m# Generate and insert data for AccountTransactionHistory table\x1B[39;00m\n' +
      "\x1B[1;32m     37\x1B[0m insert_data(\x1B[38;5;124m'\x1B[39m\x1B[38;5;124mAccountTransactionHistory\x1B[39m\x1B[38;5;124m'\x1B[39m, account_transaction_history_data)\n",
    'Cell \x1B[0;32mIn[7], line 25\x1B[0m, in \x1B[0;36minsert_data\x1B[0;34m(table_name, data)\x1B[0m\n' +
      '\x1B[1;32m     17\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21minsert_data\x1B[39m(table_name, data):\n' +
      '\x1B[1;32m     18\x1B[0m     df \x1B[38;5;241m=\x1B[39m spark\x1B[38;5;241m.\x1B[39mcreateDataFrame([Row(\x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mi) \x1B[38;5;28;01mfor\x1B[39;00m i \x1B[38;5;129;01min\x1B[39;00m data])\n' +
      '\x1B[1;32m     19\x1B[0m     df\x1B[38;5;241m.\x1B[39mwrite \\\n' +
      '\x1B[1;32m     20\x1B[0m         \x1B[38;5;241m.\x1B[39mformat(\x1B[38;5;124m"\x1B[39m\x1B[38;5;124mjdbc\x1B[39m\x1B[38;5;124m"\x1B[39m) \\\n' +
      '\x1B[1;32m     21\x1B[0m         \x1B[38;5;241m.\x1B[39moption(\x1B[38;5;124m"\x1B[39m\x1B[38;5;124murl\x1B[39m\x1B[38;5;124m"\x1B[39m, jdbc_url) \\\n' +
      '\x1B[1;32m     22\x1B[0m         \x1B[38;5;241m.\x1B[39moption(\x1B[38;5;124m"\x1B[39m\x1B[38;5;124mdbtable\x1B[39m\x1B[38;5;124m"\x1B[39m, table_name) \\\n' +
      '\x1B[1;32m     23\x1B[0m         \x1B[38;5;241m.\x1B[39moption(\x1B[38;5;124m"\x1B[39m\x1B[38;5;124muser\x1B[39m\x1B[38;5;124m"\x1B[39m, username) \\\n' +
      '\x1B[1;32m     24\x1B[0m         \x1B[38;5;241m.\x1B[39moption(\x1B[38;5;124m"\x1B[39m\x1B[38;5;124mpassword\x1B[39m\x1B[38;5;124m"\x1B[39m, password) \\\n' +
      '\x1B[0;32m---> 25\x1B[0m         \x1B[38;5;241m.\x1B[39moption(\x1B[38;5;124m"\x1B[39m\x1B[38;5;124mencrypt\x1B[39m\x1B[38;5;124m"\x1B[39m, \x1B[43mtrue\x1B[49m)\\\n' +
      '\x1B[1;32m     26\x1B[0m         \x1B[38;5;241m.\x1B[39moption(\x1B[38;5;124m"\x1B[39m\x1B[38;5;124mtrustServerCertificate\x1B[39m\x1B[38;5;124m"\x1B[39m, true)\\\n' +
      '\x1B[1;32m     27\x1B[0m         \x1B[38;5;241m.\x1B[39mmode(\x1B[38;5;124m"\x1B[39m\x1B[38;5;124mappend\x1B[39m\x1B[38;5;124m"\x1B[39m) \\\n' +
      '\x1B[1;32m     28\x1B[0m         \x1B[38;5;241m.\x1B[39msave()\n',
    "\x1B[0;31mNameError\x1B[0m: name 'true' is not defined"
  ]
}
03:09:22.355 [info] Dispose Kernel '/container/pyspark_workspace/source_code/sample_code/Test_Project_1.ipynb' associated with '/container/pyspark_workspace/source_code/sample_code/Test_Project_1.ipynb'
03:09:23.797 [warn] Cancel all remaining cells due to dead kernel
