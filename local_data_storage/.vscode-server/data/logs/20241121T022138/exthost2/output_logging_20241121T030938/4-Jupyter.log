Visual Studio Code (1.95.3, attached-container, desktop)
Jupyter Extension Version: 2024.10.0.
Python Extension Version: 2024.20.0.
Pylance Extension Version: 2024.11.2.
Platform: linux (x64).
Temp Storage folder ~/.vscode-server/data/User/globalStorage/ms-toolsai.jupyter/version-2024.10.0
Workspace folder /container/pyspark_workspace, Home = /root
03:10:25.135 [info] Starting Kernel (Python Path: /usr/local/bin/python, Unknown, 3.11.9) for '/container/pyspark_workspace/source_code/sample_code/Test_Project_1.ipynb' (disableUI=true)
03:10:29.868 [warn] Failed to get activated env vars for /usr/local/bin/python in 4755ms
03:10:29.897 [warn] Failed to get activated env vars for /usr/local/bin/python in 4037ms
03:10:30.003 [info] Process Execution: /usr/local/bin/python -c "import site;print("USER_BASE_VALUE");print(site.USER_BASE);print("USER_BASE_VALUE");"
03:10:30.178 [info] Process Execution: /usr/local/bin/python -m pip list
03:10:30.219 [info] Process Execution: /usr/local/bin/python -c "import ipykernel; print(ipykernel.__version__); print("5dc3a68c-e34e-4080-9c3e-2a532b2ccb4d"); print(ipykernel.__file__)"
03:10:30.267 [info] Process Execution: /usr/local/bin/python -m ipykernel_launcher --f=/~/.local/share/jupyter/runtime/kernel-v3fb2615f8f2c28bc7493f6fd0eadf18692e043aab.json
    > cwd: //container/pyspark_workspace/source_code/sample_code
03:10:39.338 [info] Kernel successfully started
03:10:39.451 [info] Process Execution: /usr/local/bin/python /~/.vscode-server/extensions/ms-toolsai.jupyter-2024.10.0-linux-x64/pythonFiles/printJupyterDataDir.py
03:12:23.364 [warn] Cell completed with errors iu [Error]: An error occurred while calling o69.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 0.0 failed 4 times, most recent failure: Lost task 10.3 in stage 0.0 (TID 37) (172.18.0.9 executor 0): java.sql.BatchUpdateException: String or binary data would be truncated in table 'CreditStagingDB.dbo.CustomerProductAccount', column 'PhoneNumber'. Truncated value: '+1-837-902-9077x7418'.
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2273)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
	at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4311)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4309)
	at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.sql.BatchUpdateException: String or binary data would be truncated in table 'CreditStagingDB.dbo.CustomerProductAccount', column 'PhoneNumber'. Truncated value: '+1-837-902-9077x7418'.
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2273)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more

    at n.execute (/~/.vscode-server/extensions/ms-toolsai.jupyter-2024.10.0-linux-x64/dist/extension.node.js:297:4958) {
  ename: 'Py4JJavaError',
  evalue: 'An error occurred while calling o69.save.\n' +
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 0.0 failed 4 times, most recent failure: Lost task 10.3 in stage 0.0 (TID 37) (172.18.0.9 executor 0): java.sql.BatchUpdateException: String or binary data would be truncated in table 'CreditStagingDB.dbo.CustomerProductAccount', column 'PhoneNumber'. Truncated value: '+1-837-902-9077x7418'.\n" +
    '\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2273)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n' +
    '\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n' +
    '\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n' +
    '\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n' +
    '\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n' +
    '\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n' +
    '\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n' +
    '\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n' +
    '\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n' +
    '\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n' +
    '\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n' +
    '\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n' +
    '\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n' +
    '\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n' +
    '\tat java.base/java.lang.Thread.run(Thread.java:840)\n' +
    '\n' +
    'Driver stacktrace:\n' +
    '\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n' +
    '\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n' +
    '\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n' +
    '\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n' +
    '\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n' +
    '\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n' +
    '\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n' +
    '\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n' +
    '\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n' +
    '\tat scala.Option.foreach(Option.scala:407)\n' +
    '\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n' +
    '\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n' +
    '\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n' +
    '\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n' +
    '\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n' +
    '\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n' +
    '\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n' +
    '\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n' +
    '\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n' +
    '\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n' +
    '\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)\n' +
    '\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n' +
    '\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n' +
    '\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n' +
    '\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)\n' +
    '\tat org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)\n' +
    '\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n' +
    '\tat org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4311)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
    '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
    '\tat org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4309)\n' +
    '\tat org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n' +
    '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
    '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
    '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
    '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
    '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
    '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
    '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
    '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
    '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
    '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
    '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
    '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
    '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
    '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
    '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
    '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
    '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
    '\tat java.base/java.lang.Thread.run(Thread.java:840)\n' +
    "Caused by: java.sql.BatchUpdateException: String or binary data would be truncated in table 'CreditStagingDB.dbo.CustomerProductAccount', column 'PhoneNumber'. Truncated value: '+1-837-902-9077x7418'.\n" +
    '\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2273)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n' +
    '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n' +
    '\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n' +
    '\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n' +
    '\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n' +
    '\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n' +
    '\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n' +
    '\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n' +
    '\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n' +
    '\tat org.apache.spark.util.SparkErrorUtils.tryW'... 471 more characters,
  traceback: [
    '\x1B[0;31m---------------------------------------------------------------------------\x1B[0m',
    '\x1B[0;31mPy4JJavaError\x1B[0m                             Traceback (most recent call last)',
    'Cell \x1B[0;32mIn[6], line 35\x1B[0m\n' +
      '\x1B[1;32m     33\x1B[0m customer_product_account_data, account_transaction_history_data \x1B[38;5;241m=\x1B[39m generator\x1B[38;5;241m.\x1B[39mgenerate_data(\x1B[38;5;241m20\x1B[39m)\n' +
      '\x1B[1;32m     34\x1B[0m \x1B[38;5;66;03m# Generate and insert data for CustomerProductAccount table\x1B[39;00m\n' +
      "\x1B[0;32m---> 35\x1B[0m \x1B[43minsert_data\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[38;5;124;43mCustomerProductAccount\x1B[39;49m\x1B[38;5;124;43m'\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mcustomer_product_account_data\x1B[49m\x1B[43m)\x1B[49m\n" +
      '\x1B[1;32m     36\x1B[0m \x1B[38;5;66;03m# Generate and insert data for AccountTransactionHistory table\x1B[39;00m\n' +
      "\x1B[1;32m     37\x1B[0m insert_data(\x1B[38;5;124m'\x1B[39m\x1B[38;5;124mAccountTransactionHistory\x1B[39m\x1B[38;5;124m'\x1B[39m, account_transaction_history_data)\n",
    'Cell \x1B[0;32mIn[6], line 28\x1B[0m, in \x1B[0;36minsert_data\x1B[0;34m(table_name, data)\x1B[0m\n' +
      '\x1B[1;32m     17\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21minsert_data\x1B[39m(table_name, data):\n' +
      '\x1B[1;32m     18\x1B[0m     df \x1B[38;5;241m=\x1B[39m spark\x1B[38;5;241m.\x1B[39mcreateDataFrame([Row(\x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mi) \x1B[38;5;28;01mfor\x1B[39;00m i \x1B[38;5;129;01min\x1B[39;00m data])\n' +
      '\x1B[1;32m     19\x1B[0m     \x1B[43mdf\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mwrite\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     20\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mformat\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mjdbc\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     21\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43murl\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mjdbc_url\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     22\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mdbtable\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mtable_name\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     23\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43muser\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43musername\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     24\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mpassword\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[43mpassword\x1B[49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     25\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mencrypt\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43;01mTrue\x1B[39;49;00m\x1B[43m)\x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     26\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43moption\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mtrustServerCertificate\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43;01mTrue\x1B[39;49;00m\x1B[43m)\x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[1;32m     27\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mmode\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[38;5;124;43mappend\x1B[39;49m\x1B[38;5;124;43m"\x1B[39;49m\x1B[43m)\x1B[49m\x1B[43m \x1B[49m\x1B[43m\\\x1B[49m\n' +
      '\x1B[0;32m---> 28\x1B[0m \x1B[43m        \x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1461\x1B[0m, in \x1B[0;36mDataFrameWriter.save\x1B[0;34m(self, path, format, mode, partitionBy, **options)\x1B[0m\n' +
      '\x1B[1;32m   1459\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mformat(\x1B[38;5;28mformat\x1B[39m)\n' +
      '\x1B[1;32m   1460\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m path \x1B[38;5;129;01mis\x1B[39;00m \x1B[38;5;28;01mNone\x1B[39;00m:\n' +
      '\x1B[0;32m-> 1461\x1B[0m     \x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43m_jwrite\x1B[49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43msave\x1B[49m\x1B[43m(\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1462\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m   1463\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39m_jwrite\x1B[38;5;241m.\x1B[39msave(path)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\x1B[0m, in \x1B[0;36mJavaMember.__call__\x1B[0;34m(self, *args)\x1B[0m\n' +
      '\x1B[1;32m   1316\x1B[0m command \x1B[38;5;241m=\x1B[39m proto\x1B[38;5;241m.\x1B[39mCALL_COMMAND_NAME \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1317\x1B[0m     \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mcommand_header \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1318\x1B[0m     args_command \x1B[38;5;241m+\x1B[39m\\\n' +
      '\x1B[1;32m   1319\x1B[0m     proto\x1B[38;5;241m.\x1B[39mEND_COMMAND_PART\n' +
      '\x1B[1;32m   1321\x1B[0m answer \x1B[38;5;241m=\x1B[39m \x1B[38;5;28mself\x1B[39m\x1B[38;5;241m.\x1B[39mgateway_client\x1B[38;5;241m.\x1B[39msend_command(command)\n' +
      '\x1B[0;32m-> 1322\x1B[0m return_value \x1B[38;5;241m=\x1B[39m \x1B[43mget_return_value\x1B[49m\x1B[43m(\x1B[49m\n' +
      '\x1B[1;32m   1323\x1B[0m \x1B[43m    \x1B[49m\x1B[43manswer\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mgateway_client\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mtarget_id\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;28;43mself\x1B[39;49m\x1B[38;5;241;43m.\x1B[39;49m\x1B[43mname\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m   1325\x1B[0m \x1B[38;5;28;01mfor\x1B[39;00m temp_arg \x1B[38;5;129;01min\x1B[39;00m temp_args:\n' +
      '\x1B[1;32m   1326\x1B[0m     \x1B[38;5;28;01mif\x1B[39;00m \x1B[38;5;28mhasattr\x1B[39m(temp_arg, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m_detach\x1B[39m\x1B[38;5;124m"\x1B[39m):\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\x1B[0m, in \x1B[0;36mcapture_sql_exception.<locals>.deco\x1B[0;34m(*a, **kw)\x1B[0m\n' +
      '\x1B[1;32m    177\x1B[0m \x1B[38;5;28;01mdef\x1B[39;00m \x1B[38;5;21mdeco\x1B[39m(\x1B[38;5;241m*\x1B[39ma: Any, \x1B[38;5;241m*\x1B[39m\x1B[38;5;241m*\x1B[39mkw: Any) \x1B[38;5;241m-\x1B[39m\x1B[38;5;241m>\x1B[39m Any:\n' +
      '\x1B[1;32m    178\x1B[0m     \x1B[38;5;28;01mtry\x1B[39;00m:\n' +
      '\x1B[0;32m--> 179\x1B[0m         \x1B[38;5;28;01mreturn\x1B[39;00m \x1B[43mf\x1B[49m\x1B[43m(\x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43ma\x1B[49m\x1B[43m,\x1B[49m\x1B[43m \x1B[49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[38;5;241;43m*\x1B[39;49m\x1B[43mkw\x1B[49m\x1B[43m)\x1B[49m\n' +
      '\x1B[1;32m    180\x1B[0m     \x1B[38;5;28;01mexcept\x1B[39;00m Py4JJavaError \x1B[38;5;28;01mas\x1B[39;00m e:\n' +
      '\x1B[1;32m    181\x1B[0m         converted \x1B[38;5;241m=\x1B[39m convert_exception(e\x1B[38;5;241m.\x1B[39mjava_exception)\n',
    'File \x1B[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\x1B[0m, in \x1B[0;36mget_return_value\x1B[0;34m(answer, gateway_client, target_id, name)\x1B[0m\n' +
      '\x1B[1;32m    324\x1B[0m value \x1B[38;5;241m=\x1B[39m OUTPUT_CONVERTER[\x1B[38;5;28mtype\x1B[39m](answer[\x1B[38;5;241m2\x1B[39m:], gateway_client)\n' +
      '\x1B[1;32m    325\x1B[0m \x1B[38;5;28;01mif\x1B[39;00m answer[\x1B[38;5;241m1\x1B[39m] \x1B[38;5;241m==\x1B[39m REFERENCE_TYPE:\n' +
      '\x1B[0;32m--> 326\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JJavaError(\n' +
      '\x1B[1;32m    327\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m.\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    328\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name), value)\n' +
      '\x1B[1;32m    329\x1B[0m \x1B[38;5;28;01melse\x1B[39;00m:\n' +
      '\x1B[1;32m    330\x1B[0m     \x1B[38;5;28;01mraise\x1B[39;00m Py4JError(\n' +
      '\x1B[1;32m    331\x1B[0m         \x1B[38;5;124m"\x1B[39m\x1B[38;5;124mAn error occurred while calling \x1B[39m\x1B[38;5;132;01m{0}\x1B[39;00m\x1B[38;5;132;01m{1}\x1B[39;00m\x1B[38;5;132;01m{2}\x1B[39;00m\x1B[38;5;124m. Trace:\x1B[39m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;132;01m{3}\x1B[39;00m\x1B[38;5;130;01m\\n\x1B[39;00m\x1B[38;5;124m"\x1B[39m\x1B[38;5;241m.\x1B[39m\n' +
      '\x1B[1;32m    332\x1B[0m         \x1B[38;5;28mformat\x1B[39m(target_id, \x1B[38;5;124m"\x1B[39m\x1B[38;5;124m.\x1B[39m\x1B[38;5;124m"\x1B[39m, name, value))\n',
    '\x1B[0;31mPy4JJavaError\x1B[0m: An error occurred while calling o69.save.\n' +
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 0.0 failed 4 times, most recent failure: Lost task 10.3 in stage 0.0 (TID 37) (172.18.0.9 executor 0): java.sql.BatchUpdateException: String or binary data would be truncated in table 'CreditStagingDB.dbo.CustomerProductAccount', column 'PhoneNumber'. Truncated value: '+1-837-902-9077x7418'.\n" +
      '\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2273)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n' +
      '\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n' +
      '\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n' +
      '\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n' +
      '\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n' +
      '\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n' +
      '\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n' +
      '\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n' +
      '\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n' +
      '\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n' +
      '\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n' +
      '\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n' +
      '\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n' +
      '\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n' +
      '\tat java.base/java.lang.Thread.run(Thread.java:840)\n' +
      '\n' +
      'Driver stacktrace:\n' +
      '\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n' +
      '\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n' +
      '\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n' +
      '\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n' +
      '\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n' +
      '\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n' +
      '\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n' +
      '\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n' +
      '\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n' +
      '\tat scala.Option.foreach(Option.scala:407)\n' +
      '\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n' +
      '\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n' +
      '\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n' +
      '\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n' +
      '\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n' +
      '\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n' +
      '\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n' +
      '\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n' +
      '\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n' +
      '\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n' +
      '\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)\n' +
      '\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n' +
      '\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n' +
      '\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n' +
      '\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)\n' +
      '\tat org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)\n' +
      '\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n' +
      '\tat org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4311)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
      '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
      '\tat org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4309)\n' +
      '\tat org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n' +
      '\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n' +
      '\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n' +
      '\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n' +
      '\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n' +
      '\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n' +
      '\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n' +
      '\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n' +
      '\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n' +
      '\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n' +
      '\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n' +
      '\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n' +
      '\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n' +
      '\tat py4j.Gateway.invoke(Gateway.java:282)\n' +
      '\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n' +
      '\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n' +
      '\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n' +
      '\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n' +
      '\tat java.base/java.lang.Thread.run(Thread.java:840)\n' +
      "Caused by: java.sql.BatchUpdateException: String or binary data would be truncated in table 'CreditStagingDB.dbo.CustomerProductAccount', column 'PhoneNumber'. Truncated value: '+1-837-902-9077x7418'.\n" +
      '\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2273)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n' +
      '\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n' +
      '\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n' +
      '\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n' +
      '\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n' +
      '\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n' +
      '\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n' +
      '\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n' +
      '\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n' +
      '\tat org.apache.spark'... 497 more characters
  ]
}
03:13:36.480 [warn] Disposing old controller startUsingPythonInterpreter:'.jvsc74a57bd06699b966c21a81be21759e40053de52c009daad87cbe5a3dc643468ce4ab3367.d:\Program Files\Python\python.exe.d:\Program Files\Python\python.exe.-m#ipykernel_launcher' for view = 'jupyter-notebook'
03:13:36.483 [warn] Disposing old controller startUsingPythonInterpreter:'.jvsc74a57bd06699b966c21a81be21759e40053de52c009daad87cbe5a3dc643468ce4ab3367.d:\Program Files\Python\python.exe.d:\Program Files\Python\python.exe.-m#ipykernel_launcher (Interactive)' for view = 'interactive'
