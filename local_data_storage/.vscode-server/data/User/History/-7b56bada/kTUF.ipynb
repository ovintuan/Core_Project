{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r'/container/pyspark_workspace/source_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.spark as utils_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_submit_local_jars_str:\n",
      "\t\t /container/pyspark_workspace/local_data_storage/spark/jars/delta-core_2.12-2.1.0.jar,/container/pyspark_workspace/local_data_storage/spark/jars/mssql-jdbc-12.8.1.jre11.jar,/container/pyspark_workspace/local_data_storage/spark/jars/spark-avro_2.12-3.4.0.jar,/container/pyspark_workspace/local_data_storage/spark/jars/spark-sql-kafka-0-10_2.12-3.4.0.jar\n",
      "spark_submit_jdbc_driver_str:\n",
      "\t\t /container/pyspark_workspace/local_data_storage/spark/jars/mssql-jdbc-12.8.1.jre11.jar\n",
      "spark_submit_extensions_str:\n",
      "\t\t io.delta.sql.DeltaSparkSessionExtension\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 05:55:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark, sqlContext = utils_spark.createSparkSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# current_path = os.getcwd()\n",
    "# print(current_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "# current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_path = './..'\n",
    "# os.chdir(new_path)\n",
    "# current_path = os.getcwd()\n",
    "# print(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_data.generate_dummy_data import DummyDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 05:56:53 ERROR TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job\n",
      "[Stage 1:>                                                         (0 + 8) / 16]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o96.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 48) (172.18.0.10 executor 1): com.microsoft.sqlserver.jdbc.SQLServerException: Conversion failed when converting date and/or time from character string.\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:270)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1735)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatementBatch(SQLServerPreparedStatement.java:3025)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtBatchExecCmd.doExecute(SQLServerPreparedStatement.java:2873)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7745)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4391)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:276)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:246)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2264)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)\n\tat org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4311)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4309)\n\tat org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.microsoft.sqlserver.jdbc.SQLServerException: Conversion failed when converting date and/or time from character string.\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:270)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1735)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatementBatch(SQLServerPreparedStatement.java:3025)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtBatchExecCmd.doExecute(SQLServerPreparedStatement.java:2873)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7745)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4391)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:276)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:246)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2264)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     insert_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomerProductAccount\u001b[39m\u001b[38;5;124m'\u001b[39m, customer_product_account_data)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Generate and insert data for AccountTransactionHistory table\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[43minsert_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAccountTransactionHistory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccount_transaction_history_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# time.sleep(5)  # Sleep for 5 seconds\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Stop the Spark session\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# spark.stop()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m, in \u001b[0;36minsert_data\u001b[0;34m(table_name, data)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert_data\u001b[39m(table_name, data):\n\u001b[1;32m     13\u001b[0m     df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([Row(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data])\n\u001b[1;32m     14\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjdbc_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musername\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencrypt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrustServerCertificate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o96.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 48) (172.18.0.10 executor 1): com.microsoft.sqlserver.jdbc.SQLServerException: Conversion failed when converting date and/or time from character string.\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:270)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1735)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatementBatch(SQLServerPreparedStatement.java:3025)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtBatchExecCmd.doExecute(SQLServerPreparedStatement.java:2873)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7745)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4391)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:276)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:246)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2264)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)\n\tat org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4311)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4309)\n\tat org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: com.microsoft.sqlserver.jdbc.SQLServerException: Conversion failed when converting date and/or time from character string.\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:270)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1735)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatementBatch(SQLServerPreparedStatement.java:3025)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtBatchExecCmd.doExecute(SQLServerPreparedStatement.java:2873)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7745)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4391)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:276)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:246)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2264)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:753)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Retrieve input variables from Airflow task parameters\n",
    "server_name = 'sqlserver_database_1'\n",
    "database_name = 'CreditStagingDB'\n",
    "username = 'SA'\n",
    "password = 'SQL_Server_Password_1000'\n",
    "\n",
    "\n",
    "# JDBC URL\n",
    "jdbc_url = f\"jdbc:sqlserver://{server_name};databaseName={database_name};user={username};password={password}\"\n",
    "\n",
    "# Function to insert data into a table\n",
    "def insert_data(table_name, data):\n",
    "    df = spark.createDataFrame([Row(**i) for i in data])\n",
    "    df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", username) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .option(\"encrypt\", True)\\\n",
    "        .option(\"trustServerCertificate\", True)\\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "generator = DummyDataGenerator()\n",
    "\n",
    "for _ in range(1):\n",
    "    customer_product_account_data, account_transaction_history_data = generator.generate_data(20)\n",
    "    # Generate and insert data for CustomerProductAccount table\n",
    "    insert_data('CustomerProductAccount', customer_product_account_data)\n",
    "    # Generate and insert data for AccountTransactionHistory table\n",
    "    insert_data('AccountTransactionHistory', account_transaction_history_data)\n",
    "    # time.sleep(5)  # Sleep for 5 seconds\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------+--------+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----------+-----------+--------------------+--------------------+\n",
      "|           AccountID|          CustomerID|           ProductID|LimitAmount| Balance|       TransactionID|Amount|     PaymentmethodID|   TransactionTypeID|         PaymentDate|           HistoryID|CreditScore|HistoryDate|         CreatedDate|          UpdateDate|\n",
      "+--------------------+--------------------+--------------------+-----------+--------+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----------+-----------+--------------------+--------------------+\n",
      "|aa13a71c-2de3-4f2...|3ec41809-6e4a-4bf...|85046dc2-7b33-47f...|    12453.1|42034.53|6c52e923-11d1-41f...|147.22|12edd02c-720b-432...|ae242cfc-664a-4fb...|2020-04-03T21:32:...|cd892f6b-a0c2-45e...|        306| 2021-12-12|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|cad10eda-b183-4b4...|5b58caa9-6e79-4b2...|3f4272b1-9e4b-4b6...|    42110.2|13672.21|22f53fb7-d616-4f3...|614.38|d95fa115-eec6-4a0...|a11764b7-9714-486...|2024-10-14T09:57:...|340cbdcc-964d-475...|        414| 2020-01-04|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|2243e7cd-0ebc-41b...|f36e2235-5a19-48d...|62830d0d-df59-45c...|    34715.0|44682.84|148f3646-5997-471...|288.26|0a7683dd-6799-47a...|f11ab68f-0863-400...|2021-03-26T06:34:...|698de9ca-4c3f-48a...|        832| 2020-12-17|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|2d772987-d5c0-4bc...|08a2f792-1a01-416...|9e070841-ca76-426...|    32351.5|49081.35|7e092f9d-f887-432...|659.64|dc89f5ff-9e6e-4a2...|5ca9882d-c9b9-4b7...|2024-01-28T06:35:...|015c5179-75cd-4d8...|        447| 2024-07-16|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|cf86d17c-4444-49b...|a162ff22-6bc3-407...|18a8e552-3e79-4f3...|   35037.92|12103.45|a82b6f83-0066-4e7...|395.27|bf33245b-2531-47a...|d03f0bf7-8645-417...|2020-02-03T22:21:...|df3dccf9-c345-4f9...|        621| 2021-09-22|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|060da670-f5e5-476...|aa1361d5-f331-49d...|8bf81419-fab1-486...|   18961.33| 47657.5|9f76240e-53cb-40f...|366.75|244b7d2a-e9a6-4b5...|ba585fc8-16f9-41d...|2024-09-23T21:20:...|c4009c1c-6519-4d2...|        591| 2021-06-01|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|b680eca5-bf40-422...|686cedc1-174f-4df...|d2806611-0c8f-4b0...|   34749.78|27290.26|66ed002e-1fc4-4f7...| 80.69|633cd259-c842-4e1...|00bd2db8-e45b-435...|2022-09-04T19:15:...|1efff273-9bda-4c3...|        630| 2020-11-19|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|a8439175-2217-4d6...|54f7e223-942a-4be...|2b3ce2fb-cc78-454...|   19051.68| 7038.97|71986671-8997-4c5...|150.87|4077dcd4-0d5d-4f3...|eea6757e-95c8-4f0...|2021-04-29T08:37:...|73f1dd5a-837b-4d2...|        407| 2023-05-06|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|59cae70a-3498-4d8...|0db4524c-fde4-468...|2b3ce2fb-cc78-454...|   19271.08|14012.35|065a2ee4-ee08-4d0...|307.25|0beba8b8-097d-478...|1d027421-ae18-480...|2020-10-14T20:54:...|b01e7dfd-d084-4ca...|        549| 2021-03-08|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|7c786e7b-2524-440...|03952f19-2662-4b2...|d9e6c51d-e927-446...|     7168.3| 6142.72|4d02233b-edaa-413...|681.22|25fc8730-8b7b-41b...|3f380de9-e389-499...|2024-10-28T09:27:...|2ebf32da-3a0d-407...|        486| 2023-03-27|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|bc23c6e2-780a-404...|8ef9e81d-f51f-48a...|ae8d15f0-0ccb-4a0...|   30405.04| 16140.1|8b1ac2b5-7264-406...|462.59|dd60f602-459a-463...|e25a2d59-ae66-41b...|2021-03-31T05:04:...|aa84adad-f319-474...|        469| 2021-10-27|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|a1c6dbb4-43b9-417...|012fe23e-3552-4f6...|c1b95d5d-d4c2-4ea...|    11169.8|43659.83|16104702-f138-40f...|251.29|7cb48e19-af80-4b8...|82c7a60f-1067-4fd...|2024-01-23T00:32:...|4b8c8ea0-dece-4b6...|        804| 2023-08-12|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|922b31a3-0183-43d...|e2899047-d909-417...|8edab4d4-9392-40f...|   33040.82| 40726.8|5b42a21a-6784-4b2...|109.62|ecb68289-7afd-4e4...|4ccba046-fe9d-49f...|2024-02-04T03:27:...|12e2f2dc-ea26-455...|        641| 2023-07-25|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|39d01db1-4ef8-4fb...|baf8c3be-f61c-4a0...|7f2a3f8d-8018-426...|   21938.49|31620.01|478648fb-8c4a-4c9...|134.77|526f1e19-12d8-4b6...|45e8a0c3-0e8e-451...|2020-01-14T13:29:...|0833124a-3bda-466...|        713| 2021-06-30|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|f19816ac-4cdf-4f3...|822619c9-5d59-498...|c1ab7166-f847-45e...|   27277.25|28426.93|4a729eb2-5a9f-44d...| 460.7|41632423-d9b7-447...|4b163480-bbff-4fb...|2023-01-28T11:47:...|41978261-8151-4da...|        595| 2019-12-18|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|6d01272b-4e68-4de...|62a4a95c-feac-4a4...|c1b95d5d-d4c2-4ea...|   13762.62|31201.22|19e212d7-9760-4b1...| 174.2|eb6d1013-633a-491...|6693b9bc-827b-4c0...|2024-02-16T01:17:...|812e7b5b-7a88-481...|        643| 2020-02-13|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|80ff4edd-4ec3-4ef...|6f94d4f8-5619-4d1...|f1917d3d-ef5b-4cf...|   39998.42|46984.69|2d46530b-5822-4a4...|981.17|8ebb6d78-03c2-4b4...|49e2b099-6bc8-446...|2022-12-16T20:11:...|4ddfd4d9-761f-4aa...|        499| 2022-09-10|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|e6dfa491-96a5-4aa...|07e73abd-4e3f-488...|4a2e4117-b64e-455...|    9922.68|23567.65|ec9f500c-3439-42d...|221.35|22e5bfb5-6818-43c...|01424ed0-ff24-4d5...|2022-03-26T00:24:...|97020822-6323-454...|        370| 2021-02-24|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|5757a559-7b81-436...|c3e843a2-b96a-400...|b55eeb16-c15c-4cd...|    4612.51|26843.46|151e33ff-021e-4ca...|512.14|36868ec9-9eb9-4c6...|ac7bc7c1-e3d7-4ab...|2024-02-27T14:02:...|a6295a0d-0e17-4e8...|        654| 2024-07-09|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "|36f10b4d-05b7-48e...|deea77cb-0867-4f6...|0dc27d7d-7b90-4b8...|   16460.17|46104.62|861f5783-66e3-47f...| 583.3|15074955-ed3f-410...|e1941970-320e-49f...|2020-02-19T01:56:...|c30a383d-d292-4ae...|        844| 2023-11-07|2024-11-21T05:55:...|2024-11-21T05:55:...|\n",
      "+--------------------+--------------------+--------------------+-----------+--------+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----------+-----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.createDataFrame()\n",
    "a  = [Row(**i) for i in account_transaction_history_data]\n",
    "spark.createDataFrame(a).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+-----------+-----------+-----------------+-----------------+-----------------+--------------------+--------------------+--------------------+-----------+------------+--------------------+-----------+--------+----------+----------+--------------------+--------------------+--------------------+\n",
      "|          CustomerID|FirstName|LastName|DateOfBirth|        SSN|             Ward|         District|             City|         PhoneNumber|               Email|           ProductID|ProductName|InterestRate|           AccountID|LimitAmount| Balance|  OpenDate| CloseDate|            StatusID|         CreatedDate|          UpdateDate|\n",
      "+--------------------+---------+--------+-----------+-----------+-----------------+-----------------+-----------------+--------------------+--------------------+--------------------+-----------+------------+--------------------+-----------+--------+----------+----------+--------------------+--------------------+--------------------+\n",
      "|cccfae7b-9e47-4b2...|   Gerald|    Gray| 2001-10-29|680-27-2858|        Pham Ways|     Jessicaville|         Port Amy|        234-661-0683|jmcguire@example.org|cadf1848-ae8f-4bf...|          a|        0.47|63b694bb-6f58-4c4...|    30443.2|42445.22|2021-04-27|2025-08-27|c1643f62-5bda-437...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|3a343e8c-f9fa-4bd...|   Leslie|  Warren| 2016-04-22|639-79-0558|      Brenda Mill|      Jeffreyport|       Crossville|          4573587998|aliciacraig@examp...|46985e01-848e-46f...|    perhaps|        0.82|5435c3a8-091c-41a...|   40894.99|  7377.8|2023-05-28|2027-11-30|0749d2af-f216-4ad...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|1e569c95-ec0e-48b...|  Felicia|  Obrien| 2016-01-21|209-47-7814|    Hodges Courts|   West Aaronfurt|        Jonesport|  (673)323-2589x1259|millsjulie@exampl...|eeb19f39-1bc1-4da...|      power|        0.73|11e6c5e2-f5a2-4d3...|   29269.25|17773.04|2023-02-02|2028-10-31|2a57a24a-83e2-499...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|1f006229-3a14-435...|  Leonard| Jimenez| 1941-01-20|798-64-5905|      Pratt Green|   New Brandytown|     Port Michael|   472-403-7505x6607|jessica55@example...|6eaf930b-13e5-47e...|       play|        0.72|c4212f74-c467-4d8...|   34894.17|39003.93|2024-06-30|2025-09-19|11c1a145-b3ff-49d...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|1c23d830-7729-48a...| Danielle|  Bryant| 1919-06-12|445-97-1342|    Amanda Tunnel|       Desireeton|    North Michael|  703.795.3863x10612|smithcharles@exam...|99a182aa-71b2-4e6...|       tell|        0.52|764f5661-6626-407...|   18869.01|22819.88|2020-05-18|2028-06-13|b6bb90c7-23d0-420...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|88de0964-3255-4cd...|    James|    Buck| 1964-11-28|060-53-0717|    Meyer Prairie|  Gonzalezborough|        Saraville|001-837-802-9010x993|floressherri@exam...|2b3ce2fb-cc78-454...|        try|        0.21|b778a4d1-92be-4a6...|   48187.59|43017.42|2020-04-23|2027-06-07|7116692e-c218-4cc...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|98666e24-d486-44d...|Christian|    Cook| 1976-06-20|440-24-3348|  Jennifer Canyon|      New Annland|   Harringtonside|   349.846.5457x8011|uschmidt@example.com|d6690f31-4bd6-4da...|   employee|        0.97|a36bfad0-3035-41e...|   15232.19|49775.26|2021-03-21|2029-10-12|295bb311-fd22-414...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|4f4d97e1-c0bc-4c2...|    Susan| Baldwin| 1946-06-27|702-84-1647|Gillespie Squares|Richardsonchester|  North Katherine|  330.762.0610x89590|justingreen@examp...|decae0d3-1b84-40d...|        bad|        0.43|13a1d9ce-0eaa-4ad...|   21795.51|  9850.3|2023-11-29|2025-08-13|349d0bd0-cae6-423...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|3ea6d06a-588d-443...|     Jane|    Rowe| 1945-12-15|065-39-6190|      Dana Corner|    Schneiderland|       Lake Kevin|     +1-337-850-6207|jcameron@example.com|d2806611-0c8f-4b0...|      speak|        0.67|11771ece-5a41-45b...|    5969.06|  103.41|2022-07-11|2024-11-25|5e73b894-bf44-408...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|341fbce9-ad6f-408...|  Stephen|    Pugh| 2019-12-25|094-39-3835|       Dale Locks|      Crystalfort|        Amberberg|     +1-969-524-8106|kelsey07@example.org|803c1782-6e72-411...|       find|         0.3|7292bbed-285d-456...|   29558.18|28610.33|2023-11-04|2028-10-18|8cb52259-931e-4e5...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|13d6bb9d-ce97-4e3...|  Michael|  Savage| 2004-01-29|827-59-8003|Katherine Squares|     Patriciaview|      Torreshaven| +1-470-790-1474x584|sbarnett@example.com|85046dc2-7b33-47f...|       case|        0.18|2cda64ea-6132-4a9...|   24692.06|27741.81|2022-03-18|2025-11-26|77060d3c-2fbd-4d9...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|bfa14e7a-e322-4c3...|    Alexa|  Gordon| 1938-11-03|822-08-6230|    Johnson Lodge|         Huntstad|     West Charles|   310.512.9345x5589|erikacox@example.org|5084c86a-1b9a-4e4...|    compare|        0.61|1e838c8c-a413-4e6...|   10151.23| 5845.34|2024-03-19|2025-11-11|c7968191-b8fe-46b...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|d29dd862-05f8-40a...|   Donald| Ellison| 1988-06-29|085-72-0933|  Melissa Springs|    South Kathryn|     North Autumn|  467.520.6543x77199|melissa09@example...|15f79eae-653c-459...|      seven|        0.14|5086ced3-ce12-4dd...|     5310.0|42463.12|2020-01-08|2027-11-29|f07d6bdb-5bf3-402...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|dcd722b0-3fc2-4a2...|  Jeffery| Swanson| 1987-06-12|351-73-1532| Rodriguez Lights|        Prattbury|      Comptonland|        274-570-3188|amberkirk@example...|62830d0d-df59-45c...|       walk|        0.93|e2eb3f7f-5f72-4ed...|    5542.62| 8911.35|2022-09-13|2025-04-23|fced4dfc-866d-4b6...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|bd372f6e-da98-47c...|    Susan| Shelton| 1962-06-20|176-58-6416|   Tucker Streets|       Richardton|     Lake Jessica|  277-933-0007x49864|  tony62@example.net|2b3ce2fb-cc78-454...|        try|        0.21|89839edc-ceb1-486...|   24927.85| 2726.91|2021-03-19|2028-09-12|6675514e-83f3-4aa...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|ea25b777-e830-4d5...|  Chelsea|Peterson| 1961-02-08|019-28-7702|  Matthews Plains|        Debraland|      Cooperville|   891.763.4599x9878|pratttara@example...|5ae6c5b0-16c0-4d2...|    station|         0.4|3d60aab3-1944-496...|   47917.06|48475.49|2021-08-25|2025-09-08|37239450-40d3-47c...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|84b155c8-51ce-4d3...|  Matthew|    Cruz| 1917-05-14|297-07-8638|      Hill Circle|        Greenstad|      West Daniel|+1-368-798-0517x3208|jaredthompson@exa...|62830d0d-df59-45c...|       walk|        0.93|ba10e13e-d11d-40c...|    47911.7|40542.86|2022-06-27|2027-11-22|34cc4fbc-901b-444...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|c2b8cae2-4a07-43a...|  Bridget|Williams| 1953-02-22|370-36-9585|       Craig Loop|       Petershire|      Pageborough| +1-627-697-5153x326|twilliamson@examp...|f1917d3d-ef5b-4cf...|    student|         0.3|7fa9f45a-1b1c-410...|    9654.01|46257.14|2022-07-25|2026-12-26|eca1f26c-4b48-4de...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|4f6e5ba6-be25-401...|Elizabeth| Pollard| 1911-05-06|101-39-3229|   Elliott Groves| East Erikchester|Lake Rebeccamouth|001-348-369-9269x...|portermitchell@ex...|eeb19f39-1bc1-4da...|      power|        0.73|f8d5deb1-6340-4e2...|   29610.58| 6190.08|2022-01-18|2028-12-15|e736ba46-74a2-457...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "|ea970712-0977-48c...|   Justin|   Smith| 2001-02-13|483-06-6242|Elizabeth Heights|       Torresport|     Jessicahaven|       (490)422-0742|powellbrianna@exa...|5976cf4f-1d33-448...|       rule|        0.69|65e38abe-1692-4d1...|   16631.52|28400.17|2020-04-11|2029-11-19|31aec9be-e13c-497...|2024-11-20T19:05:...|2024-11-20T19:05:...|\n",
      "+--------------------+---------+--------+-----------+-----------+-----------------+-----------------+-----------------+--------------------+--------------------+--------------------+-----------+------------+--------------------+-----------+--------+----------+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.createDataFrame()\n",
    "a  = [Row(**i) for i in customer_product_account_data]\n",
    "spark.createDataFrame(a).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# api_token = 'dapia4995328bd677127734386305a9cf6b0'\n",
    "# databrick_url = 'adb-8623833702503865.5.azuredatabrick.net'\n",
    "# headers = {\n",
    "#     'Authorization': f'Bearer {api_token}',\n",
    "#     'Content-Type': \"application/json\"\n",
    "# }\n",
    "# run_reponse = requests.get(\n",
    "#     f'https://{databrick_url}/api/2.0/secrets/acls/list',\n",
    "#     headers=headers,\n",
    "#     json = {\n",
    "#         'scope':'ls_kv_vn01_adf_vlt01'\n",
    "#     }\n",
    "# )\n",
    "# run_reponse.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"my_new_file.txt\", \"w\") as file:\n",
    "#     file.write(\"This is some text inside the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f8c0bcc0-205a-47d0-9017-950914b06b97;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.2.0/delta-spark_2.12-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.12;3.2.0!delta-spark_2.12.jar (63342ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/3.2.0/delta-storage-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;3.2.0!delta-storage.jar (779ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (2822ms)\n",
      ":: resolution report :: resolve 7546ms :: artifacts dl 66961ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f8c0bcc0-205a-47d0-9017-950914b06b97\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (6321kB/34ms)\n",
      "24/11/04 17:44:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# builder = SparkSession.builder.appName(\"MyApp\") \\\n",
    "#     .master(\"spark://spark-master:7077\")\\\n",
    "#     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "\n",
    "    \n",
    "# spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [(\"Alice\", \"34\"), (\"Bob\", \"45\"), (\"Cathy\", \"29\")]\n",
    "# df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "# df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  age\n",
       "0  Alice   25\n",
       "1    Bob   30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/31 12:43:45 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "df.write.format(\"delta\").mode('overwrite').option('overwriteSchema',True).save(\"/container/Project_1/local_data_storage/data_1/TEMP_TABLE_A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  age\n",
       "0  Alice   25\n",
       "1    Bob   30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('delta').load(\"/container/Project_1/local_data_storage/data_1/TEMP_TABLE_A\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(''' \n",
    "DESC HISTORY delta.`/container/Project_1/local_data_storage/data_1/TEMP_TABLE_A`\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612666d1ba2f41b8bd5e55deef38c555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DataGrid(auto_fit_params={'area': 'all', 'padding': 30, 'numCols': None}, corner_renderer=None, default_render"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipydatagrid import DataGrid\n",
    "DataGrid(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>201</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "      <td>202</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>103</td>\n",
       "      <td>203</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>198</td>\n",
       "      <td>298</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>199</td>\n",
       "      <td>299</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      A    B    C    D\n",
       "0     1  101  201  301\n",
       "1     2  102  202  302\n",
       "2     3  103  203  303\n",
       "..  ...  ...  ...  ...\n",
       "97   98  198  298  398\n",
       "98   99  199  299  399\n",
       "99  100  200  300  400\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': range(1, 101),\n",
    "    'B': range(101, 201),\n",
    "    'C': range(201, 301),\n",
    "    'D': range(301, 401)\n",
    "})\n",
    "\n",
    "# Set various display options\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "pd.set_option('display.show_dimensions', True)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.set_option('display.max_seq_item', 10)\n",
    "pd.set_option('display.memory_usage', True)\n",
    "\n",
    "# Display DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT * FROM ALL_TABLES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
